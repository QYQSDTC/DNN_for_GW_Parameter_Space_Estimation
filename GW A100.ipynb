{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去噪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 数据路径和加载配置 ==========\n",
    "data_folder = \"D:/data/waveforms2\"  # 你的数据目录，可按需修改\n",
    "target_snr = [\"50.00\", \"100.00\", \"200.00\"]  # 可指定加载哪些 SNR 数据\n",
    "\n",
    "# ========== 初始化自定义 Dataset ==========\n",
    "class GWDataset(Dataset):\n",
    "    def __init__(self, folder_path, snr_list=None, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        self.snr_list = snr_list or [\"50.00\", \"100.00\", \"200.00\"]\n",
    "        \n",
    "        # 检查文件夹是否存在\n",
    "        if not os.path.isdir(folder_path):\n",
    "            raise FileNotFoundError(f\"数据目录不存在: {folder_path}\")\n",
    "            \n",
    "        self.file_index = []\n",
    "        snr_pattern = re.compile(r\"_SNR(\\d+\\.\\d+)\\.h5\")\n",
    "\n",
    "        # 使用更安全的方式遍历文件\n",
    "        for fname in os.listdir(folder_path):\n",
    "            if not fname.endswith(\".h5\"):\n",
    "                continue\n",
    "                \n",
    "            full_path = os.path.join(folder_path, fname)\n",
    "            \n",
    "            # 跳过非文件项（如目录）\n",
    "            if not os.path.isfile(full_path):\n",
    "                print(f\"跳过非文件项: {full_path}\")\n",
    "                continue\n",
    "                \n",
    "            # 检查文件可读性\n",
    "            if not os.access(full_path, os.R_OK):\n",
    "                print(f\"警告: 文件不可读，跳过: {full_path}\")\n",
    "                continue\n",
    "                \n",
    "            match = snr_pattern.search(fname)\n",
    "            if match:\n",
    "                snr = match.group(1)\n",
    "                if snr in self.snr_list:\n",
    "                    # 尝试打开文件以验证完整性\n",
    "                    try:\n",
    "                        with h5py.File(full_path, \"r\") as f:\n",
    "                            # 简单验证文件结构\n",
    "                            if \"Data\" not in f:\n",
    "                                print(f\"警告: 文件缺少'Data'组，跳过: {full_path}\")\n",
    "                                continue\n",
    "                                \n",
    "                            # 检查必要属性\n",
    "                            data_group = f[\"Data\"]\n",
    "                            required_attrs = [\"mc_true\", \"phis_true\", \"thetas_true\"]\n",
    "                            if not all(attr in data_group.attrs for attr in required_attrs):\n",
    "                                print(f\"警告: 文件缺少必要属性，跳过: {full_path}\")\n",
    "                                continue\n",
    "                                \n",
    "                        # 文件验证通过，添加到索引\n",
    "                        self.file_index.append((full_path, snr))\n",
    "                        \n",
    "                    except (OSError, IOError) as e:\n",
    "                        print(f\"文件打开错误 {full_path}: {str(e)}，跳过\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"处理文件时出错 {full_path}: {str(e)}，跳过\")\n",
    "\n",
    "        if not self.file_index:\n",
    "            raise ValueError(\"没有匹配到任何指定 SNR 的文件，请检查路径或 snr_list 设置\")\n",
    "        else:\n",
    "            print(f\"成功加载 {len(self.file_index)} 个文件\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, snr = self.file_index[idx]\n",
    "        \n",
    "        try:\n",
    "            with h5py.File(file_path, \"r\") as f:\n",
    "                data_group = f[\"Data\"]\n",
    "                \n",
    "                # 处理不同的数据结构\n",
    "                if isinstance(data_group, h5py.Group):\n",
    "                    white_data = torch.tensor(data_group[\"white_Data\"][:], dtype=torch.float32)\n",
    "                    white_signal = torch.tensor(data_group[\"white_signal\"][:], dtype=torch.float32)\n",
    "                elif isinstance(data_group, h5py.Dataset):\n",
    "                    # 处理旧格式的数据集\n",
    "                    white_data = torch.tensor(data_group[0].flatten(), dtype=torch.float32)\n",
    "                    white_signal = torch.tensor(data_group[1].flatten(), dtype=torch.float32)\n",
    "                else:\n",
    "                    raise ValueError(f\"未知的数据结构: {file_path}\")\n",
    "                \n",
    "                attrs = {k: data_group.attrs[k] for k in data_group.attrs}\n",
    "                \n",
    "        except (OSError, IOError) as e:\n",
    "            # 文件读取错误时返回空数据并记录警告\n",
    "            print(f\"读取文件错误 {file_path}: {str(e)}\")\n",
    "            seq_len = 6184  # 默认序列长度\n",
    "            white_data = torch.zeros(seq_len, dtype=torch.float32)\n",
    "            white_signal = torch.zeros(seq_len, dtype=torch.float32)\n",
    "            attrs = {\n",
    "                \"mc_true\": 0.0,\n",
    "                \"phis_true\": 0.0,\n",
    "                \"thetas_true\": 0.0\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"处理文件时发生意外错误 {file_path}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "        sample = {\n",
    "            \"white_data\": white_data,\n",
    "            \"white_signal\": white_signal,\n",
    "            \"attributes\": attrs,\n",
    "            \"mc_true\": attrs.get(\"mc_true\", 0.0),\n",
    "            \"phis_true\": attrs.get(\"phis_true\", 0.0),\n",
    "            \"thetas_true\": attrs.get(\"thetas_true\", 0.0),\n",
    "            \"snr\": snr,\n",
    "            \"filename\": os.path.basename(file_path)\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# ========== 创建数据集对象 ==========\n",
    "try:\n",
    "    dataset = GWDataset(data_folder, snr_list=target_snr)\n",
    "    print(f\"共加载样本数量: {len(dataset)}\")\n",
    "except Exception as e:\n",
    "    print(f\"数据集初始化失败: {str(e)}\")\n",
    "    # 创建空数据集防止后续代码崩溃\n",
    "    class EmptyDataset(Dataset):\n",
    "        def __len__(self): return 0\n",
    "        def __getitem__(self, idx): return {}\n",
    "    dataset = EmptyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "print(\"white_data shape:\", sample[\"white_data\"].shape)\n",
    "print(\"white_signal shape:\", sample[\"white_signal\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建掩码 0表示弱信号，1表示强信号，2表示第二段弱信号\n",
    "def generate_mask(data: torch.Tensor, threshold_factor=2.0):\n",
    "    B, L = data.shape\n",
    "    mask = torch.ones_like(data, dtype=torch.long)  # 初始为全1（弱信号）\n",
    "    stds = torch.std(data, dim=1, keepdim=True)  # (B, 1)\n",
    "    thresholds = threshold_factor * stds         # 每条数据的阈值 (B, 1)\n",
    "\n",
    "    abs_data = data.abs()  # (B, L)\n",
    "    for i in range(B):\n",
    "        above_th = (abs_data[i] > thresholds[i])  # bool mask\n",
    "        strong_indices = torch.nonzero(above_th).squeeze()\n",
    "\n",
    "        if strong_indices.numel() > 0:\n",
    "            start = strong_indices[0].item()\n",
    "            end = strong_indices[-1].item() + 1\n",
    "            mask[i, start:end] = 0  # 强信号\n",
    "            mask[i, end:] = 2       # 第二段弱信号\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "def standardize_batch(data: torch.Tensor, signal: torch.Tensor, mask: torch.Tensor, amplification=10.0):\n",
    "    # 放大弱信号部分\n",
    "    signal_amplified = signal.clone()\n",
    "    signal_amplified[mask == 1] *= amplification\n",
    "\n",
    "    # 提取弱信号索引\n",
    "    weak_indices = (mask == 1)\n",
    "\n",
    "    # RobustScaler 模拟：中位数与 IQR（近似标准化）\n",
    "    weak_values = data[weak_indices].view(-1)\n",
    "    median = weak_values.median()\n",
    "    q1 = weak_values.kthvalue(int(len(weak_values) * 0.25))[0]\n",
    "    q3 = weak_values.kthvalue(int(len(weak_values) * 0.75))[0]\n",
    "    iqr = q3 - q1 + 1e-8  # 避免除零\n",
    "\n",
    "    # 标准化公式：(x - median) / IQR\n",
    "    data_std = (data - median) / iqr\n",
    "    signal_std = (signal_amplified - median) / iqr\n",
    "\n",
    "    stats = {\"median\": median.item(), \"iqr\": iqr.item()}\n",
    "    return data_std, signal_std, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集划分（训练/验证/测试）\n",
    "def split_dataset(dataset, train_ratio=0.2, val_ratio=0.2):\n",
    "    total_len = len(dataset)\n",
    "    train_len = int(total_len * train_ratio)\n",
    "    val_len = int(total_len * val_ratio)\n",
    "    test_len = total_len - train_len - val_len\n",
    "    return random_split(dataset, [train_len, val_len, test_len])\n",
    "\n",
    "# 假设 GWDataset 实例为 dataset\n",
    "train_set, val_set, test_set = split_dataset(dataset)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=16, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=16, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=7, stride=stride, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.LeakyReLU = nn.LeakyReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.LeakyReLU(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.LeakyReLU(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.LeakyReLU = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self.make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, 1)  # 输出一个值\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.LeakyReLU(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNetModel():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveUNetWithTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder（使用大卷积核，并增加层数）\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=15, padding=7), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=15, padding=7), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=15, padding=7), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=15, padding=7), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        # Transformer bottleneck\n",
    "        self.transformer_input_proj = nn.Conv1d(128, 256, kernel_size=1)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=512, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        self.transformer_output_proj = nn.Conv1d(256, 128, kernel_size=1)\n",
    "\n",
    "        # Decoder（镜像结构）\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True),\n",
    "            nn.Conv1d(128, 64, kernel_size=15, padding=7), nn.ReLU()\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True),\n",
    "            nn.Conv1d(64, 32, kernel_size=15, padding=7), nn.ReLU()\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True),\n",
    "            nn.Conv1d(32, 16, kernel_size=15, padding=7), nn.ReLU()\n",
    "        )\n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True),\n",
    "            nn.Conv1d(16, 1, kernel_size=15, padding=7)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        input_len = x.shape[-1]\n",
    "        residual = x  # 用于强信号跳跃连接\n",
    "\n",
    "        x1 = self.encoder1(x)\n",
    "        x2 = self.encoder2(x1)\n",
    "        x3 = self.encoder3(x2)\n",
    "        x4 = self.encoder4(x3)\n",
    "\n",
    "        x_trans = self.transformer_input_proj(x4).permute(0, 2, 1)\n",
    "        x_trans = self.transformer(x_trans)\n",
    "        x_trans = self.transformer_output_proj(x_trans.permute(0, 2, 1))\n",
    "\n",
    "        x = self.decoder1(x_trans)\n",
    "        x = self.decoder2(x)\n",
    "        x = self.decoder3(x)\n",
    "        x = self.decoder4(x)\n",
    "\n",
    "        # 输出与输入对齐\n",
    "        if x.shape[-1] > input_len:\n",
    "            x = x[:, :, :input_len]\n",
    "        elif x.shape[-1] < input_len:\n",
    "            x = F.pad(x, (0, input_len - x.shape[-1]))\n",
    "        \n",
    "        # 输出处理：根据掩码合成最终输出\n",
    "        output = x * (mask == 1) + residual * (mask == 0)  # 弱信号①+强信号\n",
    "        output = output * (mask != 2)  # 再将弱信号②置 0\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "denoising_model = WaveUNetWithTransformer()\n",
    "pred_model = ResNetModel()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "denoising_model.to(device)\n",
    "pred_model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(denoising_model.parameters(), lr=0.00005, weight_decay=1e-5)\n",
    "optimizer_pred = torch.optim.Adam(pred_model.parameters(), lr=0.0001, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 掩码损失函数\n",
    "def masked_loss(output, target, mask, lambda_mse=5.0, lambda_smooth=1.0, stability_penalty=1.0):\n",
    "    # ========== 主损失项 (MSE) ==========\n",
    "    loss = F.mse_loss(output, target, reduction='none')  # (B, 1, T)\n",
    "    active = (mask == 1).float()\n",
    "    core = (loss * active).sum() / (active.sum() + 1e-8)\n",
    "    # ========== 平滑项（仅对弱信号①） ==========\n",
    "    diff = output[:, :, 1:] - output[:, :, :-1]\n",
    "    mask_diff = (mask[:, :, 1:] == 1) & (mask[:, :, :-1] == 1)  # 相邻都是弱信号①\n",
    "    smooth_penalty = (diff**2 * mask_diff.float()).sum() / (mask_diff.float().sum() + 1e-8)\n",
    "    # ========== 标准差惩罚项（防塌缩，仅对弱信号①） ==========\n",
    "    weak_output = output[mask == 1]\n",
    "    std_penalty = 1.0 / (torch.std(weak_output) + 1e-4)  # 防止输出塌缩为常数\n",
    "    # ========== 总损失 ==========\n",
    "    total_loss = lambda_mse * core + lambda_smooth * smooth_penalty + stability_penalty * std_penalty\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 训练 + 验证 + 保存最优模型\n",
    "def train_model(model, train_loader, val_loader, optimizer, device, num_epochs=300, save_path=\"best_denoising_model.pt\"):\n",
    "    best_val_loss = float('inf')\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            white_data = batch[\"white_data\"].to(device)       # shape: (B, L)\n",
    "            white_signal = batch[\"white_signal\"].to(device)   # shape: (B, L)\n",
    "\n",
    "            mask = generate_mask(white_data).unsqueeze(1).to(device)  # shape: (B, 1, L)\n",
    "\n",
    "            x = white_data.unsqueeze(1)    # shape: (B, 1, L)\n",
    "            y = white_signal.unsqueeze(1)\n",
    "\n",
    "            x_std, y_std, _ = standardize_batch(x.squeeze(1), y.squeeze(1), mask.squeeze(1))  # 去通道维标准化\n",
    "            x_std = x_std.unsqueeze(1).to(device)\n",
    "            y_std = y_std.unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_std, mask)\n",
    "            loss = masked_loss(output, y_std, mask)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                white_data = batch[\"white_data\"].to(device)\n",
    "                white_signal = batch[\"white_signal\"].to(device)\n",
    "\n",
    "                mask = generate_mask(white_data).unsqueeze(1).to(device)\n",
    "\n",
    "                x = white_data.unsqueeze(1)\n",
    "                y = white_signal.unsqueeze(1)\n",
    "\n",
    "                x_std, y_std, _ = standardize_batch(x.squeeze(1), y.squeeze(1), mask.squeeze(1))\n",
    "                x_std = x_std.unsqueeze(1).to(device)\n",
    "                y_std = y_std.unsqueeze(1).to(device)\n",
    "\n",
    "                output = model(x_std, mask)\n",
    "                loss = masked_loss(output, y_std, mask)\n",
    "                val_loss += loss.item() * x.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        # 输出日志\n",
    "        with torch.no_grad():\n",
    "            output_std = output.std().item()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"{epoch+1}/{num_epochs}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}, Output Std = {output_std:.4f}, LR = {current_lr:.2e}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"  >> Best model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混合精度训练\n",
    "def train_model_amp(model, train_loader, val_loader, optimizer, device, num_epochs=100):\n",
    "    loss_fn = masked_mse_loss  # 原本定义的掩码损失函数\n",
    "    scaler = GradScaler()      # AMP 缩放器\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            x = batch[\"white_data\"].to(device, non_blocking=True)\n",
    "            y = batch[\"white_signal\"].to(device, non_blocking=True)\n",
    "            mask = generate_mask(x).unsqueeze(1).to(device)\n",
    "\n",
    "            x_std, y_std, _ = standardize_batch(x.squeeze(1), y.squeeze(1), mask.squeeze(1))\n",
    "            x_std = x_std.unsqueeze(1).to(device)\n",
    "            y_std = y_std.unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                output = model(x_std, mask)\n",
    "                loss = loss_fn(output, y_std, mask)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # 验证部分（同样支持 AMP）\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x = batch[\"white_data\"].to(device, non_blocking=True)\n",
    "                y = batch[\"white_signal\"].to(device, non_blocking=True)\n",
    "                mask = generate_mask(x).unsqueeze(1).to(device)\n",
    "\n",
    "                x_std, y_std, _ = standardize_batch(x.squeeze(1), y.squeeze(1), mask.squeeze(1))\n",
    "                x_std = x_std.unsqueeze(1).to(device)\n",
    "                y_std = y_std.unsqueeze(1).to(device)\n",
    "\n",
    "                with autocast():\n",
    "                    output = model(x_std, mask)\n",
    "                    loss = loss_fn(output, y_std, mask)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_denoising_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "train_model(denoising_model, train_loader, val_loader, optimizer, device, num_epochs=120)\n",
    "# train_model_amp(denoising_model, train_loader, val_loader, optimizer, device, num_epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "def test_model_and_save_labels_batched(model, test_loader, device, output_dir=\"pt_chunks\", base_name=\"denoised_batch\", target_names=[\"mc_true\", \"phis_true\", \"thetas_true\"]):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            white_data = batch[\"white_data\"].to(device)\n",
    "            mask = generate_mask(white_data).unsqueeze(1).to(device)\n",
    "            x = white_data.unsqueeze(1)\n",
    "\n",
    "            # 标准化处理\n",
    "            x_std, _, _ = standardize_batch(x.squeeze(1), x.squeeze(1), mask.squeeze(1))\n",
    "            x_std = x_std.unsqueeze(1).to(device)\n",
    "\n",
    "            # 去噪输出\n",
    "            output = model(x_std, mask).cpu()\n",
    "\n",
    "            # 提取三类标签并组合为 (B, 3)\n",
    "            mc = torch.tensor(batch[\"mc_true\"], dtype=torch.float32).view(-1, 1)\n",
    "            phis = torch.tensor(batch[\"phis_true\"], dtype=torch.float32).view(-1, 1)\n",
    "            thetas = torch.tensor(batch[\"thetas_true\"], dtype=torch.float32).view(-1, 1)\n",
    "            targets = torch.cat([mc, phis, thetas], dim=1)\n",
    "\n",
    "            # 保存文件\n",
    "            file_path = os.path.join(output_dir, f\"{base_name}_{batch_count:04d}.pt\")\n",
    "            torch.save({\n",
    "                \"denoised\": output,\n",
    "                \"targets\": targets,\n",
    "                \"target_names\": target_names\n",
    "            }, file_path)\n",
    "            \n",
    "            torch.cuda.empty_cache()  # 保存后立即释放缓存\n",
    "\n",
    "            print(f\"[Saved] {file_path} ← {output.shape[0]} samples\")\n",
    "            batch_count += 1\n",
    "\n",
    "    print(f\"[Done] 共保存 {batch_count} 个批次文件于: {output_dir}\")\n",
    "\n",
    "\n",
    "# 加载并评估最佳模型\n",
    "denoising_model.load_state_dict(torch.load(\"best_denoising_model.pt\"))\n",
    "test_model_and_save_labels_batched(model=denoising_model, test_loader=test_loader, device=device, output_dir=\"pt_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仅反归一化弱信号部分\n",
    "def selective_inverse_transform(signal, mask, stats, amplification=5.0):\n",
    "    signal = signal.copy()\n",
    "    weak_indices = np.where(mask == 1)[0]\n",
    "    if len(weak_indices) > 0:\n",
    "        # 先反标准化\n",
    "        signal[weak_indices] = signal[weak_indices] * stats[\"iqr\"] + stats[\"median\"]\n",
    "        # 再恢复原始比例\n",
    "        signal[weak_indices] /= amplification\n",
    "    return signal\n",
    "\n",
    "# 可视化去噪效果（输入、预测、纯信号）\n",
    "def visualize_denoising_subplots(model, test_loader, device, sample_index=0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(test_loader))\n",
    "        white_data = batch[\"white_data\"].to(device)\n",
    "        white_signal = batch[\"white_signal\"].to(device)\n",
    "        mask = generate_mask(white_data).unsqueeze(1).to(device)\n",
    "\n",
    "        x = white_data.unsqueeze(1)\n",
    "        y = white_signal.unsqueeze(1)\n",
    "\n",
    "        # 标准化 + 获取统计量\n",
    "        x_std, y_std, stats = standardize_batch(x.squeeze(1), y.squeeze(1), mask.squeeze(1))\n",
    "        x_std = x_std.unsqueeze(1).to(device)\n",
    "        y_std = y_std.unsqueeze(1).to(device)\n",
    "\n",
    "        output = model(x_std, mask).squeeze(1).cpu().numpy()\n",
    "\n",
    "    # 选择样本\n",
    "    input_signal = x[sample_index].squeeze().cpu().numpy()\n",
    "    denoised_signal = output[sample_index]\n",
    "    clean_signal = y[sample_index].squeeze().cpu().numpy()\n",
    "    signal_mask = mask[sample_index].squeeze().cpu().numpy()\n",
    "\n",
    "    # 反归一化\n",
    "    input_signal = selective_inverse_transform(input_signal, signal_mask, stats, amplification=1.0)\n",
    "    denoised_signal = selective_inverse_transform(denoised_signal, signal_mask, stats, amplification=10.0)\n",
    "    clean_signal = selective_inverse_transform(clean_signal, signal_mask, stats, amplification=10.0)\n",
    "\n",
    "    # 绘图\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(15, 9), sharex=True)\n",
    "    axs[0].plot(input_signal, color='orange')\n",
    "    axs[0].set_title(\"Noisy Input\")\n",
    "    axs[1].plot(denoised_signal, color='green')\n",
    "    axs[1].set_title(\"Denoised Output\")\n",
    "    axs[2].plot(clean_signal, color='blue')\n",
    "    axs[2].set_title(\"Ground Truth Signal\")\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_ylim(-0.5, 0.5)\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_denoising_subplots(denoising_model, test_loader, device=device, sample_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyDenoisedDataset(Dataset):\n",
    "    def __init__(self, chunk_folder, label_idx=0):\n",
    "        self.chunk_paths = sorted([\n",
    "            os.path.join(chunk_folder, f)\n",
    "            for f in os.listdir(chunk_folder)\n",
    "            if f.endswith(\".pt\")\n",
    "        ])\n",
    "        self.label_idx = label_idx\n",
    "        self.index_map = []\n",
    "\n",
    "        # 预先构建 (file_id, local_id) 映射表\n",
    "        for file_idx, file_path in enumerate(self.chunk_paths):\n",
    "            data = torch.load(file_path, map_location=\"cpu\")\n",
    "            count = data[\"denoised\"].shape[0]\n",
    "            self.index_map.extend([(file_idx, i) for i in range(count)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, local_idx = self.index_map[idx]\n",
    "        file_path = self.chunk_paths[file_idx]\n",
    "        data = torch.load(file_path, map_location=\"cpu\")\n",
    "\n",
    "        signal = data[\"denoised\"][local_idx].float()     # shape: (1, L)\n",
    "        label = data[\"targets\"][local_idx, self.label_idx].unsqueeze(0).float()  # shape: (1,)\n",
    "\n",
    "        return {\"signal\": signal, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mc_model(model, train_loader, val_loader, optimizer, scaler, device, num_epochs=150):\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            x = batch[\"signal\"].to(device)  # (B, 1, L)\n",
    "            y = batch[\"label\"].to(device)   # (B, 1)\n",
    "\n",
    "            # 归一化标签\n",
    "            y_scaled = scaler.transform(y.cpu().numpy())\n",
    "            y_scaled = torch.tensor(y_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y_scaled)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "        # ===== 验证 =====\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x = batch[\"signal\"].to(device)\n",
    "                y = batch[\"label\"].to(device)\n",
    "                y_scaled = scaler.transform(y.cpu().numpy())\n",
    "                y_scaled = torch.tensor(y_scaled, dtype=torch.float32).to(device)\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y_scaled)\n",
    "                val_loss += loss.item() * x.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_model_Mc.pt\")\n",
    "            print(\">> Best model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混合精度训练\n",
    "def train_mc_model_amp(model, train_loader, val_loader, optimizer, scaler, device, num_epochs=150):\n",
    "    model.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    amp_scaler = GradScaler()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            x = batch[\"signal\"].to(device, non_blocking=True)\n",
    "            y = batch[\"label\"].cpu().numpy()\n",
    "            y_scaled = scaler.transform(y)\n",
    "            y_scaled = torch.tensor(y_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                pred = model(x)\n",
    "                loss = loss_fn(pred, y_scaled)\n",
    "\n",
    "            amp_scaler.scale(loss).backward()\n",
    "            amp_scaler.step(optimizer)\n",
    "            amp_scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # === 验证阶段 ===\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x = batch[\"signal\"].to(device, non_blocking=True)\n",
    "                y = batch[\"label\"].cpu().numpy()\n",
    "                y_scaled = scaler.transform(y)\n",
    "                y_scaled = torch.tensor(y_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "                with autocast():\n",
    "                    pred = model(x)\n",
    "                    loss = loss_fn(pred, y_scaled)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model_Mc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mc_model(model, test_loader, scaler, device):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x = batch[\"signal\"].to(device)\n",
    "            y = batch[\"label\"].cpu().numpy()\n",
    "            pred = model(x).cpu().numpy()\n",
    "            preds.append(pred)\n",
    "            trues.append(y)\n",
    "\n",
    "    preds = scaler.inverse_transform(np.vstack(preds)).flatten()\n",
    "    trues = np.vstack(trues).flatten()\n",
    "\n",
    "    # === 输出评估指标 ===\n",
    "    mse = mean_squared_error(trues, preds)\n",
    "    r2 = r2_score(trues, preds)\n",
    "    print(\"Test MSE:\", mse)\n",
    "    print(\"Test R² :\", r2)\n",
    "\n",
    "    # === 散点图 ===\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(trues, preds, alpha=0.5)\n",
    "    plt.plot([trues.min(), trues.max()], [trues.min(), trues.max()], 'r--')\n",
    "    plt.xlabel(\"True Value\")\n",
    "    plt.ylabel(\"Predicted Value\")\n",
    "    plt.title(\"Prediction Scatter Plot\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # === 误差直方图 ===\n",
    "    errors = preds - trues\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(errors, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    plt.axvline(x=0, color='red', linestyle='--', label='Zero Error')\n",
    "    plt.xlabel(\"Prediction Error\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Prediction Error Histogram\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # === Violin Plot ===\n",
    "    data = pd.DataFrame({\n",
    "        \"Type\": [\"True\"] * len(trues) + [\"Predicted\"] * len(preds),\n",
    "        \"Value\": np.concatenate([trues, preds])\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.violinplot(x=\"Type\", y=\"Value\", data=data, inner=\"quartile\", palette=\"muted\")\n",
    "    plt.title(\"Distribution of True vs Predicted Values\")\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 参数配置 ----------\n",
    "chunk_dir = \"pt_chunks\"         # 分批保存的 .pt 文件路径\n",
    "label_idx = 0                   # 0=mc_true, 1=phis_true, 2=thetas_true\n",
    "batch_size = 16\n",
    "model_save_path = \"best_model_Mc.pt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- 构建数据集 ----------\n",
    "dataset = LazyDenoisedDataset(chunk_dir, label_idx=label_idx)\n",
    "print(f\"[INFO] 样本总数: {len(dataset)}\")\n",
    "\n",
    "# ---------- 提取所有标签进行归一化 ----------\n",
    "all_labels = torch.stack([dataset[i][\"label\"] for i in range(len(dataset))])\n",
    "label_scaler = StandardScaler()\n",
    "label_scaler.fit(all_labels.numpy())\n",
    "\n",
    "# ---------- 数据划分 ----------\n",
    "n = len(dataset)\n",
    "train_len = int(n * 0.6)\n",
    "val_len = int(n * 0.2)\n",
    "test_len = n - train_len - val_len\n",
    "train_set, val_set, test_set = random_split(dataset, [train_len, val_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_set, batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "# ---------- 初始化模型 ----------\n",
    "model = ResNetModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# ---------- 训练 ----------\n",
    "train_mc_model(model, train_loader, val_loader, optimizer, label_scaler, device)\n",
    "# train_mc_model_amp(pred_model, train_loader, val_loader, optimizer, label_scaler, device)\n",
    "\n",
    "# ---------- 测试 ----------\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "evaluate_mc_model(model, test_loader, label_scaler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FND",
   "language": "python",
   "name": "fnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
