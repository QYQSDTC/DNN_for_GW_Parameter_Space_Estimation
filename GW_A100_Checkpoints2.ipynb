{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b085485",
   "metadata": {},
   "source": [
    "# Training on A100 with checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4bfe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import platform\n",
    "\n",
    "# æ ¹æ®æ“ä½œç³»ç»Ÿè®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "system = platform.system()\n",
    "\n",
    "if system == \"Darwin\":  # macOS\n",
    "    plt.rcParams['font.sans-serif'] = ['PingFang SC', 'Heiti SC', 'STHeiti', 'Arial Unicode MS']\n",
    "elif system == \"Linux\":\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'WenQuanYi Micro Hei', 'Noto Sans CJK SC']\n",
    "elif system == \"Windows\":\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'KaiTi', 'FangSong']\n",
    "else:\n",
    "    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "\n",
    "# è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"å·²è®¾ç½®{system}ç³»ç»Ÿçš„ä¸­æ–‡å­—ä½“æ”¯æŒ\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc184a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== åˆå§‹åŒ–è‡ªå®šä¹‰ Dataset ==========\n",
    "class GWDataset(Dataset):\n",
    "    def __init__(self, folder_path, snr_list=None, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        self.snr_list = snr_list or [\"50.00\", \"100.00\", \"200.00\"]\n",
    "        \n",
    "        # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨\n",
    "        if not os.path.isdir(folder_path):\n",
    "            raise FileNotFoundError(f\"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {folder_path}\")\n",
    "            \n",
    "        self.file_index = []\n",
    "        snr_pattern = re.compile(r\"_SNR(\\d+\\.\\d+)\\.h5\")\n",
    "\n",
    "        # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼éå†æ–‡ä»¶\n",
    "        for fname in os.listdir(folder_path):\n",
    "            if not fname.endswith(\".h5\"):\n",
    "                continue\n",
    "                \n",
    "            full_path = os.path.join(folder_path, fname)\n",
    "            \n",
    "            # è·³è¿‡éæ–‡ä»¶é¡¹ï¼ˆå¦‚ç›®å½•ï¼‰\n",
    "            if not os.path.isfile(full_path):\n",
    "                print(f\"è·³è¿‡éæ–‡ä»¶é¡¹: {full_path}\")\n",
    "                continue\n",
    "                \n",
    "            # æ£€æŸ¥æ–‡ä»¶å¯è¯»æ€§\n",
    "            if not os.access(full_path, os.R_OK):\n",
    "                print(f\"è­¦å‘Š: æ–‡ä»¶ä¸å¯è¯»ï¼Œè·³è¿‡: {full_path}\")\n",
    "                continue\n",
    "                \n",
    "            match = snr_pattern.search(fname)\n",
    "            if match:\n",
    "                snr = match.group(1)\n",
    "                if snr in self.snr_list:\n",
    "                    # å°è¯•æ‰“å¼€æ–‡ä»¶ä»¥éªŒè¯å®Œæ•´æ€§\n",
    "                    try:\n",
    "                        with h5py.File(full_path, \"r\") as f:\n",
    "                            # ç®€å•éªŒè¯æ–‡ä»¶ç»“æ„\n",
    "                            if \"Data\" not in f:\n",
    "                                print(f\"è­¦å‘Š: æ–‡ä»¶ç¼ºå°‘'Data'ç»„ï¼Œè·³è¿‡: {full_path}\")\n",
    "                                continue\n",
    "                                \n",
    "                            # æ£€æŸ¥å¿…è¦å±æ€§\n",
    "                            data_group = f[\"Data\"]\n",
    "                            required_attrs = [\"mc_true\", \"phis_true\", \"thetas_true\"]\n",
    "                            if not all(attr in data_group.attrs for attr in required_attrs):\n",
    "                                print(f\"è­¦å‘Š: æ–‡ä»¶ç¼ºå°‘å¿…è¦å±æ€§ï¼Œè·³è¿‡: {full_path}\")\n",
    "                                continue\n",
    "                                \n",
    "                        # æ–‡ä»¶éªŒè¯é€šè¿‡ï¼Œæ·»åŠ åˆ°ç´¢å¼•\n",
    "                        self.file_index.append((full_path, snr))\n",
    "                        \n",
    "                    except (OSError, IOError) as e:\n",
    "                        print(f\"æ–‡ä»¶æ‰“å¼€é”™è¯¯ {full_path}: {str(e)}ï¼Œè·³è¿‡\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {full_path}: {str(e)}ï¼Œè·³è¿‡\")\n",
    "\n",
    "        if not self.file_index:\n",
    "            raise ValueError(\"æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æŒ‡å®š SNR çš„æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ– snr_list è®¾ç½®\")\n",
    "        else:\n",
    "            print(f\"æˆåŠŸåŠ è½½ {len(self.file_index)} ä¸ªæ–‡ä»¶\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, snr = self.file_index[idx]\n",
    "        \n",
    "        try:\n",
    "            with h5py.File(file_path, \"r\") as f:\n",
    "                data_group = f[\"Data\"]\n",
    "                \n",
    "                # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„\n",
    "                if isinstance(data_group, h5py.Group):\n",
    "                    white_data = torch.tensor(data_group[\"white_Data\"][:], dtype=torch.float32)\n",
    "                    white_signal = torch.tensor(data_group[\"white_signal\"][:], dtype=torch.float32)\n",
    "                elif isinstance(data_group, h5py.Dataset):\n",
    "                    # å¤„ç†æ—§æ ¼å¼çš„æ•°æ®é›†\n",
    "                    white_data = torch.tensor(data_group[0].flatten(), dtype=torch.float32)\n",
    "                    white_signal = torch.tensor(data_group[1].flatten(), dtype=torch.float32)\n",
    "                else:\n",
    "                    raise ValueError(f\"æœªçŸ¥çš„æ•°æ®ç»“æ„: {file_path}\")\n",
    "                \n",
    "                attrs = {k: data_group.attrs[k] for k in data_group.attrs}\n",
    "                \n",
    "        except (OSError, IOError) as e:\n",
    "            # æ–‡ä»¶è¯»å–é”™è¯¯æ—¶è¿”å›ç©ºæ•°æ®å¹¶è®°å½•è­¦å‘Š\n",
    "            print(f\"è¯»å–æ–‡ä»¶é”™è¯¯ {file_path}: {str(e)}\")\n",
    "            seq_len = 6184  # é»˜è®¤åºåˆ—é•¿åº¦\n",
    "            white_data = torch.zeros(seq_len, dtype=torch.float32)\n",
    "            white_signal = torch.zeros(seq_len, dtype=torch.float32)\n",
    "            attrs = {\n",
    "                \"mc_true\": 0.0,\n",
    "                \"phis_true\": 0.0,\n",
    "                \"thetas_true\": 0.0\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ {file_path}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "        sample = {\n",
    "            \"white_data\": white_data,\n",
    "            \"white_signal\": white_signal,\n",
    "            \"attributes\": attrs,\n",
    "            \"mc_true\": attrs.get(\"mc_true\", 0.0),\n",
    "            \"phis_true\": attrs.get(\"phis_true\", 0.0),\n",
    "            \"thetas_true\": attrs.get(\"thetas_true\", 0.0),\n",
    "            \"snr\": snr,\n",
    "            \"filename\": os.path.basename(file_path)\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009daab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºæ©ç  1è¡¨ç¤ºå¼±ä¿¡å·ï¼Œ0è¡¨ç¤ºå¼ºä¿¡å·ï¼Œ2è¡¨ç¤ºç¬¬äºŒæ®µå¼±ä¿¡å·\n",
    "def generate_mask(data: torch.Tensor, threshold_factor=5.0):\n",
    "    B, L = data.shape\n",
    "    mask = torch.ones_like(data, dtype=torch.long)  # åˆå§‹ä¸ºå…¨1ï¼ˆå¼±ä¿¡å·ï¼‰\n",
    "    stds = torch.std(data, dim=1, keepdim=True)  # (B, 1)\n",
    "    thresholds = threshold_factor * stds         # æ¯æ¡æ•°æ®çš„é˜ˆå€¼ (B, 1)\n",
    "\n",
    "    abs_data = data.abs()  # (B, L)\n",
    "    for i in range(B):\n",
    "        above_th = (abs_data[i] > thresholds[i])  # bool mask\n",
    "        strong_indices = torch.nonzero(above_th).squeeze()\n",
    "\n",
    "        if strong_indices.numel() > 0:\n",
    "            # å¤„ç†0ç»´å¼ é‡çš„æƒ…å†µï¼ˆåªæœ‰ä¸€ä¸ªå¼ºä¿¡å·ç‚¹ï¼‰\n",
    "            if strong_indices.dim() == 0:\n",
    "                start = end = strong_indices.item()\n",
    "            else:\n",
    "                start = strong_indices[0].item()\n",
    "                end = strong_indices[-1].item()\n",
    "            \n",
    "            mask[i, start:end+1] = 0  # å¼ºä¿¡å·ï¼ˆåŒ…å«endç‚¹ï¼‰\n",
    "            mask[i, end+1:] = 2       # ç¬¬äºŒæ®µå¼±ä¿¡å·\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e9d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ‡å‡†åŒ–\n",
    "def standardize_batch(data: torch.Tensor, signal: torch.Tensor, mask: torch.Tensor, amplification=1.0):\n",
    "    # æ”¾å¤§å¼±ä¿¡å·éƒ¨åˆ†\n",
    "    signal_amplified = signal.clone()\n",
    "    signal_amplified[mask == 1] *= amplification\n",
    "\n",
    "    # æå–å¼±ä¿¡å·ç´¢å¼•\n",
    "    weak_indices = (mask == 1)\n",
    "\n",
    "    # RobustScaler æ¨¡æ‹Ÿï¼šä¸­ä½æ•°ä¸ IQRï¼ˆè¿‘ä¼¼æ ‡å‡†åŒ–ï¼‰\n",
    "    weak_values = data[weak_indices].view(-1)\n",
    "    median = weak_values.median()\n",
    "    q1 = weak_values.kthvalue(int(len(weak_values) * 0.25))[0]\n",
    "    q3 = weak_values.kthvalue(int(len(weak_values) * 0.75))[0]\n",
    "    iqr = q3 - q1 + 1e-8  # é¿å…é™¤é›¶\n",
    "\n",
    "    # æ ‡å‡†åŒ–å…¬å¼ï¼š(x - median) / IQR\n",
    "    data_std = (data - median) / iqr\n",
    "    signal_std = (signal_amplified - median) / iqr\n",
    "\n",
    "    stats = {\"median\": median.item(), \"iqr\": iqr.item()}\n",
    "    return data_std, signal_std, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e4401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®é›†åˆ’åˆ†ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•ï¼‰\n",
    "def split_dataset(dataset, train_ratio=0.2, val_ratio=0.2):\n",
    "    total_len = len(dataset)\n",
    "    train_len = int(total_len * train_ratio)\n",
    "    val_len = int(total_len * val_ratio)\n",
    "    test_len = total_len - train_len - val_len\n",
    "    return random_split(dataset, [train_len, val_len, test_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=7, stride=stride, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.LeakyReLU = nn.LeakyReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.LeakyReLU(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.LeakyReLU(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.LeakyReLU = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self.make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, 1)  # è¾“å‡ºä¸€ä¸ªå€¼\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.LeakyReLU(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNetModel():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce33c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveUNetWithTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoderï¼ˆä½¿ç”¨å¤§å·ç§¯æ ¸ï¼Œå¹¶å¢åŠ å±‚æ•°ï¼‰\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=15, padding=7), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=15, padding=7), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=15, padding=7), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=15, padding=7), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        # Transformer bottleneck\n",
    "        self.transformer_input_proj = nn.Conv1d(128, 256, kernel_size=1)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=512, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        self.transformer_output_proj = nn.Conv1d(256, 128, kernel_size=1)\n",
    "\n",
    "        # Decoderï¼ˆé•œåƒç»“æ„ï¼‰\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True),\n",
    "            nn.Conv1d(128, 64, kernel_size=15, padding=7), nn.ReLU()\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True),\n",
    "            nn.Conv1d(64, 32, kernel_size=15, padding=7), nn.ReLU()\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True),\n",
    "            nn.Conv1d(32, 16, kernel_size=15, padding=7), nn.ReLU()\n",
    "        )\n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True),\n",
    "            nn.Conv1d(16, 1, kernel_size=15, padding=7)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        input_len = x.shape[-1]\n",
    "        residual = x  # ç”¨äºå¼ºä¿¡å·è·³è·ƒè¿æ¥\n",
    "\n",
    "        x1 = self.encoder1(x)\n",
    "        x2 = self.encoder2(x1)\n",
    "        x3 = self.encoder3(x2)\n",
    "        x4 = self.encoder4(x3)\n",
    "\n",
    "        x_trans = self.transformer_input_proj(x4).permute(0, 2, 1)\n",
    "        x_trans = self.transformer(x_trans)\n",
    "        x_trans = self.transformer_output_proj(x_trans.permute(0, 2, 1))\n",
    "\n",
    "        x = self.decoder1(x_trans)\n",
    "        x = self.decoder2(x)\n",
    "        x = self.decoder3(x)\n",
    "        x = self.decoder4(x)\n",
    "\n",
    "        # è¾“å‡ºä¸è¾“å…¥å¯¹é½\n",
    "        if x.shape[-1] > input_len:\n",
    "            x = x[:, :, :input_len]\n",
    "        elif x.shape[-1] < input_len:\n",
    "            x = F.pad(x, (0, input_len - x.shape[-1]))\n",
    "        \n",
    "        # è¾“å‡ºå¤„ç†ï¼šæ ¹æ®æ©ç åˆæˆæœ€ç»ˆè¾“å‡º\n",
    "        # output = x * (mask == 1) + residual * (mask == 0)  # å¼±ä¿¡å·â‘ +å¼ºä¿¡å·\n",
    "        # output = output * (mask != 2)  # å†å°†å¼±ä¿¡å·â‘¡ç½® 0\n",
    "        output = (x * (mask == 1).float() + residual * (mask == 0).float()) * (mask != 2).float()\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28dd927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ©ç æŸå¤±å‡½æ•°\n",
    "def masked_loss(output, target, mask, lambda_mse=5.0, lambda_smooth=1.0, stability_penalty=1.0):\n",
    "    # ========== ä¸»æŸå¤±é¡¹ (MSE) ==========\n",
    "    loss = F.mse_loss(output, target, reduction='none')  # (B, 1, T)\n",
    "    active = (mask == 1).float()\n",
    "    core = (loss * active).sum() / (active.sum() + 1e-8)\n",
    "    # ========== å¹³æ»‘é¡¹ï¼ˆä»…å¯¹å¼±ä¿¡å·â‘ ï¼‰ ==========\n",
    "    diff = output[:, :, 1:] - output[:, :, :-1]\n",
    "    mask_diff = (mask[:, :, 1:] == 1) & (mask[:, :, :-1] == 1)  # ç›¸é‚»éƒ½æ˜¯å¼±ä¿¡å·â‘ \n",
    "    smooth_penalty = (diff**2 * mask_diff.float()).sum() / (mask_diff.float().sum() + 1e-8)\n",
    "    # ========== æ ‡å‡†å·®æƒ©ç½šé¡¹ï¼ˆé˜²å¡Œç¼©ï¼Œä»…å¯¹å¼±ä¿¡å·â‘ ï¼‰ ==========\n",
    "    weak_output = output[mask == 1]\n",
    "    std_penalty = 1.0 / (torch.std(weak_output) + 1e-4)  # é˜²æ­¢è¾“å‡ºå¡Œç¼©ä¸ºå¸¸æ•°\n",
    "    # ========== æ€»æŸå¤± ==========\n",
    "    total_loss = lambda_mse * core + lambda_smooth * smooth_penalty + stability_penalty * std_penalty\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# denoising def selective_inverse_transform(signal, mask, stats, amplification=10.0):\n",
    "    signal = signal.copy()\n",
    "    signal = signal * stats[\"iqr\"] + stats[\"median\"]  # æ‰€æœ‰åŒºåŸŸåæ ‡å‡†åŒ–\n",
    "    signal[mask == 1] /= amplification                # ä»…å¼±ä¿¡å·å†é™¤ä»¥æ”¾å¤§å€æ•°\n",
    "    return signal\n",
    "\n",
    "# å¯è§†åŒ–å»å™ªæ•ˆæœï¼ˆè¾“å…¥ã€é¢„æµ‹ã€çº¯ä¿¡å·ï¼‰\n",
    "def visualize_denoising_subplots(model, test_loader, device, sample_index=0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(test_loader))\n",
    "        white_data = batch[\"white_data\"].to(device)\n",
    "        white_signal = batch[\"white_signal\"].to(device)\n",
    "        mask = generate_mask(white_data).unsqueeze(1).to(device)\n",
    "        print(mask[sample_index])\n",
    "\n",
    "        x = white_data.unsqueeze(1)\n",
    "        y = white_signal.unsqueeze(1)\n",
    "\n",
    "        # æ ‡å‡†åŒ– + è·å–ç»Ÿè®¡é‡\n",
    "        x_std, y_std, stats = standardize_batch(x.squeeze(1), y.squeeze(1), mask.squeeze(1))\n",
    "        x_std = x_std.unsqueeze(1).to(device)\n",
    "        y_std = y_std.unsqueeze(1).to(device)\n",
    "\n",
    "        output = model(x_std, mask).squeeze(1).cpu().numpy()\n",
    "        print(output[sample_index])\n",
    "\n",
    "    # é€‰æ‹©æ ·æœ¬\n",
    "    input_signal = x[sample_index].squeeze().cpu().numpy()\n",
    "    denoised_signal = output[sample_index]\n",
    "    clean_signal = y[sample_index].squeeze().cpu().numpy()\n",
    "    signal_mask = mask[sample_index].squeeze().cpu().numpy()\n",
    "\n",
    "    # åå½’ä¸€åŒ–\n",
    "    input_signal = selective_inverse_transform(input_signal, signal_mask, stats, amplification=1.0)\n",
    "    denoised_signal = selective_inverse_transform(denoised_signal, signal_mask, stats, amplification=1.0)\n",
    "    clean_signal = selective_inverse_transform(clean_signal, signal_mask, stats, amplification=1.0)\n",
    "\n",
    "    # ç»˜å›¾\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(15, 9), sharex=True)\n",
    "    axs[0].plot(input_signal, color='orange')\n",
    "    axs[0].set_title(\"Noisy Input\")\n",
    "    axs[1].plot(denoised_signal, color='green')\n",
    "    axs[1].set_title(\"Denoised Output\")\n",
    "    axs[2].plot(clean_signal, color='blue')\n",
    "    axs[2].set_title(\"Ground Truth Signal\")\n",
    "\n",
    "    for ax in axs:\n",
    "        # ax.set_ylim(-1, 1)\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def test_model_and_save_labels_batched(model, test_loader, device, output_dir=\"pt_chunks\", base_name=\"denoised_batch\", target_names=[\"mc_true\", \"phis_true\", \"thetas_true\"]):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            white_data = batch[\"white_data\"].to(device)\n",
    "            mask = generate_mask(white_data).unsqueeze(1).to(device)\n",
    "            x = white_data.unsqueeze(1)\n",
    "\n",
    "            # æ ‡å‡†åŒ–å¤„ç†\n",
    "            x_std, _, _ = standardize_batch(x.squeeze(1), x.squeeze(1), mask.squeeze(1))\n",
    "            x_std = x_std.unsqueeze(1).to(device)\n",
    "\n",
    "            # å»å™ªè¾“å‡º\n",
    "            output = model(x_std, mask).cpu()\n",
    "\n",
    "            # æå–ä¸‰ç±»æ ‡ç­¾å¹¶ç»„åˆä¸º (B, 3)\n",
    "            mc = torch.tensor(batch[\"mc_true\"], dtype=torch.float32).view(-1, 1)\n",
    "            phis = torch.tensor(batch[\"phis_true\"], dtype=torch.float32).view(-1, 1)\n",
    "            thetas = torch.tensor(batch[\"thetas_true\"], dtype=torch.float32).view(-1, 1)\n",
    "            targets = torch.cat([mc, phis, thetas], dim=1)\n",
    "\n",
    "            # ä¿å­˜æ–‡ä»¶\n",
    "            file_path = os.path.join(output_dir, f\"{base_name}_{batch_count:04d}.pt\")\n",
    "            torch.save({\n",
    "                \"denoised\": output,\n",
    "                \"targets\": targets,\n",
    "                \"target_names\": target_names\n",
    "            }, file_path)\n",
    "            \n",
    "            torch.cuda.empty_cache()  # ä¿å­˜åç«‹å³é‡Šæ”¾ç¼“å­˜\n",
    "\n",
    "            print(f\"[Saved] {file_path} â† {output.shape[0]} samples\")\n",
    "            batch_count += 1\n",
    "\n",
    "    print(f\"[Done] å…±ä¿å­˜ {batch_count} ä¸ªæ‰¹æ¬¡æ–‡ä»¶äº: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd1e3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†å™¨å·²åˆå§‹åŒ–\n"
     ]
    }
   ],
   "source": [
    "## å®Œæ•´çš„æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ç³»ç»Ÿ\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"å®Œæ•´çš„æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½ç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir=\"checkpoints\"):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†å™¨\n",
    "        Args:\n",
    "            save_dir: ä¿å­˜æ£€æŸ¥ç‚¹çš„ç›®å½•\n",
    "        \"\"\"\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    def save_checkpoint(self, model, optimizer, scheduler, epoch, train_loss, val_loss, \n",
    "                       best_val_loss, model_config=None, training_config=None, \n",
    "                       checkpoint_name=None):\n",
    "        \"\"\"\n",
    "        ä¿å­˜å®Œæ•´çš„æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "        Args:\n",
    "            model: æ¨¡å‹å®ä¾‹\n",
    "            optimizer: ä¼˜åŒ–å™¨\n",
    "            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "            epoch: å½“å‰è½®æ¬¡\n",
    "            train_loss: è®­ç»ƒæŸå¤±\n",
    "            val_loss: éªŒè¯æŸå¤±\n",
    "            best_val_loss: æœ€ä½³éªŒè¯æŸå¤±\n",
    "            model_config: æ¨¡å‹é…ç½®å­—å…¸\n",
    "            training_config: è®­ç»ƒé…ç½®å­—å…¸\n",
    "            checkpoint_name: æ£€æŸ¥ç‚¹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ\n",
    "        \"\"\"\n",
    "        if checkpoint_name is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            checkpoint_name = f\"checkpoint_epoch_{epoch}_{timestamp}.pt\"\n",
    "        \n",
    "        checkpoint_path = os.path.join(self.save_dir, checkpoint_name)\n",
    "        \n",
    "        # å‡†å¤‡ä¿å­˜çš„æ•°æ®\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_config': model_config or {},\n",
    "            'training_config': training_config or {}\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜æ£€æŸ¥ç‚¹\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        # ä¿å­˜å…ƒæ•°æ®åˆ°JSONæ–‡ä»¶ï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰\n",
    "        metadata = {\n",
    "            'checkpoint_name': checkpoint_name,\n",
    "            'epoch': epoch,\n",
    "            'train_loss': float(train_loss),\n",
    "            'val_loss': float(val_loss),\n",
    "            'best_val_loss': float(best_val_loss),\n",
    "            'timestamp': checkpoint['timestamp'],\n",
    "            'model_config': model_config or {},\n",
    "            'training_config': training_config or {}\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(self.save_dir, checkpoint_name.replace('.pt', '_metadata.json'))\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path, model, optimizer=None, scheduler=None, device='cpu'):\n",
    "        \"\"\"\n",
    "        åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "        Args:\n",
    "            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„\n",
    "            model: æ¨¡å‹å®ä¾‹\n",
    "            optimizer: ä¼˜åŒ–å™¨ï¼ˆå¯é€‰ï¼‰\n",
    "            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰\n",
    "            device: è®¾å¤‡\n",
    "        Returns:\n",
    "            dict: åŒ…å«åŠ è½½ä¿¡æ¯çš„å­—å…¸\n",
    "        \"\"\"\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}\")\n",
    "        \n",
    "        print(f\"ğŸ“‚ æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}\")\n",
    "        \n",
    "        # åŠ è½½æ£€æŸ¥ç‚¹\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        \n",
    "        # åŠ è½½æ¨¡å‹çŠ¶æ€\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚æœæä¾›ï¼‰\n",
    "        if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(\"âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½\")\n",
    "        \n",
    "        # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€ï¼ˆå¦‚æœæä¾›ï¼‰\n",
    "        if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            print(\"âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€å·²åŠ è½½\")\n",
    "        \n",
    "        # æå–è®­ç»ƒä¿¡æ¯\n",
    "        loaded_info = {\n",
    "            'epoch': checkpoint.get('epoch', 0),\n",
    "            'train_loss': checkpoint.get('train_loss', float('inf')),\n",
    "            'val_loss': checkpoint.get('val_loss', float('inf')),\n",
    "            'best_val_loss': checkpoint.get('best_val_loss', float('inf')),\n",
    "            'timestamp': checkpoint.get('timestamp', 'unknown'),\n",
    "            'model_config': checkpoint.get('model_config', {}),\n",
    "            'training_config': checkpoint.get('training_config', {})\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ!\")\n",
    "        print(f\"   - è½®æ¬¡: {loaded_info['epoch']}\")\n",
    "        print(f\"   - éªŒè¯æŸå¤±: {loaded_info['val_loss']:.6f}\")\n",
    "        print(f\"   - æœ€ä½³éªŒè¯æŸå¤±: {loaded_info['best_val_loss']:.6f}\")\n",
    "        print(f\"   - ä¿å­˜æ—¶é—´: {loaded_info['timestamp']}\")\n",
    "        \n",
    "        return loaded_info\n",
    "    \n",
    "    def save_best_model(self, model, val_loss, model_name=\"best_model.pt\"):\n",
    "        \"\"\"\n",
    "        ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåªä¿å­˜æ¨¡å‹æƒé‡ï¼‰\n",
    "        Args:\n",
    "            model: æ¨¡å‹å®ä¾‹\n",
    "            val_loss: éªŒè¯æŸå¤±\n",
    "            model_name: æ¨¡å‹æ–‡ä»¶å\n",
    "        \"\"\"\n",
    "        model_path = os.path.join(self.save_dir, model_name)\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }, model_path)\n",
    "        print(f\"ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {model_path} (éªŒè¯æŸå¤±: {val_loss:.6f})\")\n",
    "        return model_path\n",
    "    \n",
    "    def load_best_model(self, model, model_name=\"best_model.pt\", device='cpu'):\n",
    "        \"\"\"\n",
    "        åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "        Args:\n",
    "            model: æ¨¡å‹å®ä¾‹\n",
    "            model_name: æ¨¡å‹æ–‡ä»¶å\n",
    "            device: è®¾å¤‡\n",
    "        \"\"\"\n",
    "        model_path = os.path.join(self.save_dir, model_name)\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"æœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}\")\n",
    "        \n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        print(f\"ğŸ† æœ€ä½³æ¨¡å‹å·²åŠ è½½: {model_path}\")\n",
    "        if 'val_loss' in checkpoint:\n",
    "            print(f\"   - éªŒè¯æŸå¤±: {checkpoint['val_loss']:.6f}\")\n",
    "        if 'timestamp' in checkpoint:\n",
    "            print(f\"   - ä¿å­˜æ—¶é—´: {checkpoint['timestamp']}\")\n",
    "    \n",
    "    def list_checkpoints(self):\n",
    "        \"\"\"åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ£€æŸ¥ç‚¹\"\"\"\n",
    "        checkpoints = []\n",
    "        for file in os.listdir(self.save_dir):\n",
    "            if file.endswith('.pt') and not file.startswith('best_'):\n",
    "                metadata_file = file.replace('.pt', '_metadata.json')\n",
    "                metadata_path = os.path.join(self.save_dir, metadata_file)\n",
    "                \n",
    "                if os.path.exists(metadata_path):\n",
    "                    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "                        metadata = json.load(f)\n",
    "                    checkpoints.append(metadata)\n",
    "                else:\n",
    "                    # å¦‚æœæ²¡æœ‰å…ƒæ•°æ®æ–‡ä»¶ï¼Œåªæ˜¾ç¤ºæ–‡ä»¶å\n",
    "                    checkpoints.append({'checkpoint_name': file, 'epoch': 'unknown'})\n",
    "        \n",
    "        # æŒ‰è½®æ¬¡æ’åº\n",
    "        checkpoints.sort(key=lambda x: x.get('epoch', 0))\n",
    "        return checkpoints\n",
    "    \n",
    "    def cleanup_old_checkpoints(self, keep_last_n=5):\n",
    "        \"\"\"\n",
    "        æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹ï¼Œåªä¿ç•™æœ€è¿‘çš„Nä¸ª\n",
    "        Args:\n",
    "            keep_last_n: ä¿ç•™çš„æ£€æŸ¥ç‚¹æ•°é‡\n",
    "        \"\"\"\n",
    "        checkpoints = self.list_checkpoints()\n",
    "        if len(checkpoints) <= keep_last_n:\n",
    "            print(f\"å½“å‰æ£€æŸ¥ç‚¹æ•°é‡({len(checkpoints)})æœªè¶…è¿‡ä¿ç•™æ•°é‡({keep_last_n})ï¼Œæ— éœ€æ¸…ç†\")\n",
    "            return\n",
    "        \n",
    "        # åˆ é™¤æ—§çš„æ£€æŸ¥ç‚¹\n",
    "        to_remove = checkpoints[:-keep_last_n]\n",
    "        for checkpoint in to_remove:\n",
    "            checkpoint_name = checkpoint['checkpoint_name']\n",
    "            checkpoint_path = os.path.join(self.save_dir, checkpoint_name)\n",
    "            metadata_path = checkpoint_path.replace('.pt', '_metadata.json')\n",
    "            \n",
    "            # åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                os.remove(checkpoint_path)\n",
    "                print(f\"ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {checkpoint_name}\")\n",
    "            \n",
    "            # åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶\n",
    "            if os.path.exists(metadata_path):\n",
    "                os.remove(metadata_path)\n",
    "        \n",
    "        print(f\"âœ… æ¸…ç†å®Œæˆï¼Œä¿ç•™äº†æœ€è¿‘çš„ {keep_last_n} ä¸ªæ£€æŸ¥ç‚¹\")\n",
    "\n",
    "# åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨å®ä¾‹\n",
    "checkpoint_manager = ModelCheckpoint(save_dir=\"model_checkpoints\")\n",
    "print(\"âœ… æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†å™¨å·²åˆå§‹åŒ–\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ”¹è¿›çš„è®­ç»ƒå‡½æ•°ï¼Œæ”¯æŒä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ\n",
    "def train_model_with_checkpoints(model, train_loader, val_loader, optimizer, device, \n",
    "                                num_epochs=120, checkpoint_manager=None, \n",
    "                                resume_from_checkpoint=None, save_every_n_epochs=10,\n",
    "                                model_config=None, training_config=None):\n",
    "    \"\"\"\n",
    "    å¸¦æ£€æŸ¥ç‚¹æ”¯æŒçš„è®­ç»ƒå‡½æ•°\n",
    "    Args:\n",
    "        model: æ¨¡å‹å®ä¾‹\n",
    "        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨\n",
    "        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨\n",
    "        optimizer: ä¼˜åŒ–å™¨\n",
    "        device: è®¾å¤‡\n",
    "        num_epochs: æ€»è®­ç»ƒè½®æ¬¡\n",
    "        checkpoint_manager: æ£€æŸ¥ç‚¹ç®¡ç†å™¨\n",
    "        resume_from_checkpoint: è¦æ¢å¤çš„æ£€æŸ¥ç‚¹è·¯å¾„\n",
    "        save_every_n_epochs: æ¯éš”å¤šå°‘è½®æ¬¡ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹\n",
    "        model_config: æ¨¡å‹é…ç½®\n",
    "        training_config: è®­ç»ƒé…ç½®\n",
    "    \"\"\"\n",
    "    \n",
    "    # åˆå§‹åŒ–å˜é‡\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "    \n",
    "    # å¦‚æœéœ€è¦ä»æ£€æŸ¥ç‚¹æ¢å¤\n",
    "    if resume_from_checkpoint and checkpoint_manager:\n",
    "        try:\n",
    "            loaded_info = checkpoint_manager.load_checkpoint(\n",
    "                resume_from_checkpoint, model, optimizer, scheduler, device\n",
    "            )\n",
    "            start_epoch = loaded_info['epoch']\n",
    "            best_val_loss = loaded_info['best_val_loss']\n",
    "            print(f\"ğŸ”„ ä»è½®æ¬¡ {start_epoch} ç»§ç»­è®­ç»ƒï¼Œå½“å‰æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}\")\n",
    "            print(\"ğŸ†• ä»å¤´å¼€å§‹è®­ç»ƒ\")\n",
    "            start_epoch = 0\n",
    "            best_val_loss = float('inf')\n",
    "    \n",
    "    # å‡†å¤‡è®­ç»ƒå’Œæ¨¡å‹é…ç½®ä¿¡æ¯\n",
    "    if training_config is None:\n",
    "        training_config = {\n",
    "            'num_epochs': num_epochs,\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "            'optimizer': type(optimizer).__name__,\n",
    "            'scheduler': type(scheduler).__name__,\n",
    "            'device': str(device),\n",
    "            'batch_size': train_loader.batch_size,\n",
    "            'train_dataset_size': len(train_loader.dataset),\n",
    "            'val_dataset_size': len(val_loader.dataset)\n",
    "        }\n",
    "    \n",
    "    if model_config is None:\n",
    "        model_config = {\n",
    "            'model_type': type(model).__name__,\n",
    "            'total_params': sum(p.numel() for p in model.parameters()),\n",
    "            'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        }\n",
    "    \n",
    "    print(f\"ğŸš€ å¼€å§‹è®­ç»ƒ:\")\n",
    "    print(f\"   - æ¨¡å‹: {model_config['model_type']}\")\n",
    "    print(f\"   - æ€»å‚æ•°é‡: {model_config['total_params']:,}\")\n",
    "    print(f\"   - å¯è®­ç»ƒå‚æ•°: {model_config['trainable_params']:,}\")\n",
    "    print(f\"   - è®¾å¤‡: {device}\")\n",
    "    print(f\"   - è®­ç»ƒè½®æ¬¡: {start_epoch} -> {num_epochs}\")\n",
    "    print(f\"   - å­¦ä¹ ç‡: {training_config['learning_rate']}\")\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # è®­ç»ƒé˜¶æ®µ\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            white_data = batch[\"white_data\"].to(device)\n",
    "            white_signal = batch[\"white_signal\"].to(device)\n",
    "\n",
    "            mask = generate_mask(white_data).unsqueeze(1).to(device)\n",
    "\n",
    "            x = white_data.unsqueeze(1)\n",
    "            y = white_signal.unsqueeze(1)\n",
    "\n",
    "            x_std, y_std, _ = standardize_batch(x.squeeze(1), y.squeeze(1), mask.squeeze(1))\n",
    "            x_std = x_std.unsqueeze(1).to(device)\n",
    "            y_std = y_std.unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_std, mask)\n",
    "            loss = masked_loss(output, y_std, mask)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "        \n",
    "        train_loss /= train_batches\n",
    "\n",
    "        # éªŒè¯é˜¶æ®µ\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                white_data = batch[\"white_data\"].to(device)\n",
    "                white_signal = batch[\"white_signal\"].to(device)\n",
    "\n",
    "                mask = generate_mask(white_data).unsqueeze(1).to(device)\n",
    "\n",
    "                x = white_data.unsqueeze(1)\n",
    "                y = white_signal.unsqueeze(1)\n",
    "\n",
    "                x_std, y_std, _ = standardize_batch(x.squeeze(1), y.squeeze(1), mask.squeeze(1))\n",
    "                x_std = x_std.unsqueeze(1).to(device)\n",
    "                y_std = y_std.unsqueeze(1).to(device)\n",
    "\n",
    "                output = model(x_std, mask)\n",
    "                loss = masked_loss(output, y_std, mask)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        val_loss /= val_batches\n",
    "\n",
    "        # å­¦ä¹ ç‡è°ƒåº¦\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # è¾“å‡ºè®­ç»ƒä¿¡æ¯\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        output_std = output.std().item() if 'output' in locals() else 0.0\n",
    "        \n",
    "        print(f\"{epoch+1:3d}/{num_epochs}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}, \"\n",
    "              f\"Output Std = {output_std:.4f}, LR = {current_lr:.2e}\")\n",
    "\n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if checkpoint_manager:\n",
    "                checkpoint_manager.save_best_model(model, val_loss, \"best_denoising_model.pt\")\n",
    "            else:\n",
    "                torch.save(model.state_dict(), \"best_denoising_model.pt\")\n",
    "            print(\"  ğŸ† Best model saved!\")\n",
    "\n",
    "        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹\n",
    "        if checkpoint_manager and (epoch + 1) % save_every_n_epochs == 0:\n",
    "            checkpoint_manager.save_checkpoint(\n",
    "                model, optimizer, scheduler, epoch + 1, train_loss, val_loss, \n",
    "                best_val_loss, model_config, training_config\n",
    "            )\n",
    "        \n",
    "        # æ¯50è½®æ¸…ç†ä¸€æ¬¡æ—§æ£€æŸ¥ç‚¹\n",
    "        if checkpoint_manager and (epoch + 1) % 50 == 0:\n",
    "            checkpoint_manager.cleanup_old_checkpoints(keep_last_n=5)\n",
    "    \n",
    "    # è®­ç»ƒç»“æŸåä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹\n",
    "    if checkpoint_manager:\n",
    "        final_checkpoint = checkpoint_manager.save_checkpoint(\n",
    "            model, optimizer, scheduler, num_epochs, train_loss, val_loss, \n",
    "            best_val_loss, model_config, training_config, \n",
    "            checkpoint_name=f\"final_checkpoint_epoch_{num_epochs}.pt\"\n",
    "        )\n",
    "        print(f\"ğŸ¯ è®­ç»ƒå®Œæˆ! æœ€ç»ˆæ£€æŸ¥ç‚¹: {final_checkpoint}\")\n",
    "    \n",
    "    print(f\"âœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}\")\n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c25f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®ç”¨å·¥å…·å‡½æ•°\n",
    "def list_available_checkpoints(checkpoint_manager):\n",
    "    \"\"\"åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ£€æŸ¥ç‚¹\"\"\"\n",
    "    print(\"ğŸ“‹ å¯ç”¨çš„æ£€æŸ¥ç‚¹:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    checkpoints = checkpoint_manager.list_checkpoints()\n",
    "    if not checkpoints:\n",
    "        print(\"   æš‚æ— æ£€æŸ¥ç‚¹\")\n",
    "        return\n",
    "    \n",
    "    for i, checkpoint in enumerate(checkpoints):\n",
    "        print(f\"{i+1:2d}. {checkpoint['checkpoint_name']}\")\n",
    "        print(f\"     è½®æ¬¡: {checkpoint.get('epoch', 'unknown')}\")\n",
    "        print(f\"     éªŒè¯æŸå¤±: {checkpoint.get('val_loss', 'unknown')}\")\n",
    "        print(f\"     ä¿å­˜æ—¶é—´: {checkpoint.get('timestamp', 'unknown')}\")\n",
    "        print()\n",
    "\n",
    "def create_new_training_session(data_folder, target_snr=[\"200.00\"], model_type=\"WaveUNet\"):\n",
    "    \"\"\"åˆ›å»ºæ–°çš„è®­ç»ƒä¼šè¯\"\"\"\n",
    "    print(\"ğŸ†• åˆ›å»ºæ–°çš„è®­ç»ƒä¼šè¯...\")\n",
    "    \n",
    "    # é‡æ–°åŠ è½½æ•°æ®é›†\n",
    "    dataset = GWDataset(data_folder, snr_list=target_snr)\n",
    "    print(f\"ğŸ“Š æ•°æ®é›†åŠ è½½å®Œæˆ: {len(dataset)} ä¸ªæ ·æœ¬\")\n",
    "    \n",
    "    # æ•°æ®é›†åˆ’åˆ†\n",
    "    train_set, val_set, test_set = split_dataset(dataset)\n",
    "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=24, pin_memory=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=24, pin_memory=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=32, shuffle=False, num_workers=24, pin_memory=True)\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å‹\n",
    "    if model_type == \"WaveUNet\":\n",
    "        model = WaveUNetWithTransformer()\n",
    "    elif model_type == \"ResNet\":\n",
    "        model = ResNetModel()\n",
    "    else:\n",
    "        raise ValueError(f\"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}\")\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "    \n",
    "    return model, optimizer, train_loader, val_loader, test_loader\n",
    "\n",
    "def continue_training_from_checkpoint(checkpoint_path, data_folder=None, target_snr=None):\n",
    "    \"\"\"ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ\"\"\"\n",
    "    print(\"ğŸ”„ ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ...\")\n",
    "    \n",
    "    # å¦‚æœæä¾›äº†æ–°çš„æ•°æ®ï¼Œé‡æ–°åŠ è½½æ•°æ®é›†\n",
    "    if data_folder and target_snr:\n",
    "        print(\"ğŸ“Š ä½¿ç”¨æ–°çš„è®­ç»ƒæ•°æ®...\")\n",
    "        model, optimizer, train_loader, val_loader, test_loader = create_new_training_session(\n",
    "            data_folder, target_snr\n",
    "        )\n",
    "    else:\n",
    "        print(\"ğŸ“Š ä½¿ç”¨åŸæœ‰çš„è®­ç»ƒæ•°æ®...\")\n",
    "        # è¿™é‡Œåº”è¯¥ä½¿ç”¨ä¹‹å‰çš„æ•°æ®åŠ è½½å™¨ï¼Œå¦‚æœæ²¡æœ‰åˆ™éœ€è¦é‡æ–°åˆ›å»º\n",
    "        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬é‡æ–°åˆ›å»º\n",
    "        model, optimizer, train_loader, val_loader, test_loader = create_new_training_session(\n",
    "            \"/home/ud202180035/DNN_for_GW_Parameter_Space_Estimation/waveforms2\", [\"200.00\"]\n",
    "        )\n",
    "    \n",
    "    return model, optimizer, train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c9027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ª!\n",
      "\n",
      "ğŸ“– ä½¿ç”¨æ–¹æ³•:\n",
      "1. example_train_from_scratch() - ä»å¤´å¼€å§‹è®­ç»ƒ\n",
      "2. example_continue_from_checkpoint() - ä»æ£€æŸ¥ç‚¹ç»§ç»­\n",
      "3. example_continue_with_new_data() - æ›´æ¢æ•°æ®ç»§ç»­è®­ç»ƒ\n",
      "4. quick_train(data_folder, target_snr, num_epochs) - å¿«é€Ÿè®­ç»ƒ\n",
      "5. list_available_checkpoints(checkpoint_manager) - æŸ¥çœ‹æ£€æŸ¥ç‚¹\n"
     ]
    }
   ],
   "source": [
    "## ä½¿ç”¨ç¤ºä¾‹å’Œè¯´æ˜\n",
    "\n",
    "\"\"\"\n",
    "ä½¿ç”¨æŒ‡å—ï¼šæ¨¡å‹ä¿å­˜å’ŒåŠ è½½ç³»ç»Ÿ\n",
    "\n",
    "1. ä»å¤´å¼€å§‹è®­ç»ƒï¼š\n",
    "   - åˆ›å»ºæ–°çš„è®­ç»ƒä¼šè¯\n",
    "   - ä½¿ç”¨ train_model_with_checkpoints è¿›è¡Œè®­ç»ƒ\n",
    "   - ç³»ç»Ÿä¼šè‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹å’Œå®šæœŸæ£€æŸ¥ç‚¹\n",
    "\n",
    "2. ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒï¼š\n",
    "   - æŸ¥çœ‹å¯ç”¨æ£€æŸ¥ç‚¹\n",
    "   - é€‰æ‹©è¦ç»§ç»­çš„æ£€æŸ¥ç‚¹\n",
    "   - å¯ä»¥ä½¿ç”¨ç›¸åŒæ•°æ®æˆ–æ›´æ¢æ–°æ•°æ®ç»§ç»­è®­ç»ƒ\n",
    "\n",
    "3. ä½¿ç”¨ä¸åŒæ•°æ®ç»§ç»­è®­ç»ƒï¼š\n",
    "   - åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "   - æ›´æ¢è®­ç»ƒæ•°æ®é›†\n",
    "   - ç»§ç»­è®­ç»ƒä»¥é€‚åº”æ–°æ•°æ®\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# ç¤ºä¾‹ 1: ä»å¤´å¼€å§‹è®­ç»ƒ\n",
    "# ============================================================================\n",
    "def example_train_from_scratch():\n",
    "    \"\"\"ç¤ºä¾‹ï¼šä»å¤´å¼€å§‹è®­ç»ƒ\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ç¤ºä¾‹ 1: ä»å¤´å¼€å§‹è®­ç»ƒ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # åˆ›å»ºæ–°çš„è®­ç»ƒä¼šè¯\n",
    "    model, optimizer, train_loader, val_loader, test_loader = create_new_training_session(\n",
    "        data_folder=\"/home/ud202180035/DNN_for_GW_Parameter_Space_Estimation/waveforms2\",\n",
    "        target_snr=[\"200.00\"],\n",
    "        model_type=\"WaveUNet\"\n",
    "    )\n",
    "    \n",
    "    # å¼€å§‹è®­ç»ƒï¼ˆå¸¦æ£€æŸ¥ç‚¹æ”¯æŒï¼‰\n",
    "    best_loss = train_model_with_checkpoints(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=200,  # å¯æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "        checkpoint_manager=checkpoint_manager,\n",
    "        save_every_n_epochs=10  # æ¯10è½®ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹\n",
    "    )\n",
    "    \n",
    "    return model, best_loss\n",
    "\n",
    "# ============================================================================\n",
    "# ç¤ºä¾‹ 2: ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒï¼ˆç›¸åŒæ•°æ®ï¼‰\n",
    "# ============================================================================\n",
    "def example_continue_from_checkpoint():\n",
    "    \"\"\"ç¤ºä¾‹ï¼šä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ç¤ºä¾‹ 2: ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # æŸ¥çœ‹å¯ç”¨æ£€æŸ¥ç‚¹\n",
    "    list_available_checkpoints(checkpoint_manager)\n",
    "    \n",
    "    # é€‰æ‹©ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼ˆè¿™é‡Œä½¿ç”¨ç¤ºä¾‹è·¯å¾„ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ›¿æ¢ï¼‰\n",
    "    checkpoint_path = \"model_checkpoints/checkpoint_epoch_50_20231201_123456.pt\"  # ç¤ºä¾‹è·¯å¾„\n",
    "    \n",
    "    # å¦‚æœæ£€æŸ¥ç‚¹å­˜åœ¨ï¼Œä»è¯¥ç‚¹ç»§ç»­è®­ç»ƒ\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        # åˆ›å»ºæ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨\n",
    "        model, optimizer, train_loader, val_loader, test_loader = continue_training_from_checkpoint(\n",
    "            checkpoint_path\n",
    "        )\n",
    "        \n",
    "        # ç»§ç»­è®­ç»ƒ\n",
    "        best_loss = train_model_with_checkpoints(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            num_epochs=100,  # è®­ç»ƒåˆ°ç¬¬100è½®\n",
    "            checkpoint_manager=checkpoint_manager,\n",
    "            resume_from_checkpoint=checkpoint_path,\n",
    "            save_every_n_epochs=10\n",
    "        )\n",
    "        \n",
    "        return model, best_loss\n",
    "    else:\n",
    "        print(f\"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}\")\n",
    "        return None, None\n",
    "\n",
    "# ============================================================================\n",
    "# ç¤ºä¾‹ 3: æ›´æ¢æ•°æ®ç»§ç»­è®­ç»ƒ\n",
    "# ============================================================================\n",
    "def example_continue_with_new_data():\n",
    "    \"\"\"ç¤ºä¾‹ï¼šä½¿ç”¨æ–°æ•°æ®ç»§ç»­è®­ç»ƒ\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ç¤ºä¾‹ 3: æ›´æ¢æ•°æ®ç»§ç»­è®­ç»ƒ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # åŠ è½½ä¹‹å‰è®­ç»ƒçš„æœ€ä½³æ¨¡å‹\n",
    "    model = WaveUNetWithTransformer()\n",
    "    model.to(device)\n",
    "    \n",
    "    try:\n",
    "        # åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡\n",
    "        checkpoint_manager.load_best_model(model, \"best_denoising_model.pt\", device)\n",
    "        \n",
    "        # ä½¿ç”¨æ–°çš„æ•°æ®é›†ï¼ˆç¤ºä¾‹ï¼šä¸åŒçš„SNRï¼‰\n",
    "        new_data_folder = \"/home/ud202180035/DNN_for_GW_Parameter_Space_Estimation/waveforms_new\"  # æ–°æ•°æ®è·¯å¾„\n",
    "        new_target_snr = [\"100.00\", \"150.00\"]  # æ–°çš„SNRè®¾ç½®\n",
    "        \n",
    "        # åˆ›å»ºæ–°çš„æ•°æ®åŠ è½½å™¨\n",
    "        new_dataset = GWDataset(new_data_folder, snr_list=new_target_snr)\n",
    "        new_train_set, new_val_set, new_test_set = split_dataset(new_dataset)\n",
    "        \n",
    "        new_train_loader = DataLoader(new_train_set, batch_size=32, shuffle=True, num_workers=24, pin_memory=True)\n",
    "        new_val_loader = DataLoader(new_val_set, batch_size=32, shuffle=False, num_workers=24, pin_memory=True)\n",
    "        \n",
    "        # åˆ›å»ºæ–°çš„ä¼˜åŒ–å™¨ï¼ˆå¯ä»¥ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒï¼‰\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-6)  # æ›´å°çš„å­¦ä¹ ç‡\n",
    "        \n",
    "        # ç»§ç»­è®­ç»ƒ\n",
    "        best_loss = train_model_with_checkpoints(\n",
    "            model=model,\n",
    "            train_loader=new_train_loader,\n",
    "            val_loader=new_val_loader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            num_epochs=30,  # å¾®è°ƒè½®æ¬¡\n",
    "            checkpoint_manager=checkpoint_manager,\n",
    "            save_every_n_epochs=5,\n",
    "            training_config={\n",
    "                'fine_tuning': True,\n",
    "                'original_data': \"waveforms2\",\n",
    "                'new_data': \"waveforms_new\",\n",
    "                'new_snr': new_target_snr\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return model, best_loss\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ æ— æ³•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ============================================================================\n",
    "# å®é™…ä½¿ç”¨æ—¶çš„ç®€åŒ–æ¥å£\n",
    "# ============================================================================\n",
    "def quick_train(data_folder, target_snr=[\"200.00\"], num_epochs=50, resume_checkpoint=None):\n",
    "    \"\"\"å¿«é€Ÿè®­ç»ƒæ¥å£\"\"\"\n",
    "    print(\"ğŸš€ å¿«é€Ÿè®­ç»ƒæ¨¡å¼\")\n",
    "    \n",
    "    # åˆ›å»ºè®­ç»ƒä¼šè¯\n",
    "    model, optimizer, train_loader, val_loader, test_loader = create_new_training_session(\n",
    "        data_folder, target_snr\n",
    "    )\n",
    "    \n",
    "    # å¼€å§‹è®­ç»ƒ\n",
    "    best_loss = train_model_with_checkpoints(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        checkpoint_manager=checkpoint_manager,\n",
    "        resume_from_checkpoint=resume_checkpoint,\n",
    "        save_every_n_epochs=max(1, num_epochs // 5)  # ä¿å­˜5æ¬¡æ£€æŸ¥ç‚¹\n",
    "    )\n",
    "    \n",
    "    return model, best_loss\n",
    "\n",
    "print(\"âœ… æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ª!\")\n",
    "print(\"\\nğŸ“– ä½¿ç”¨æ–¹æ³•:\")\n",
    "print(\"1. example_train_from_scratch() - ä»å¤´å¼€å§‹è®­ç»ƒ\")\n",
    "print(\"2. example_continue_from_checkpoint() - ä»æ£€æŸ¥ç‚¹ç»§ç»­\")\n",
    "print(\"3. example_continue_with_new_data() - æ›´æ¢æ•°æ®ç»§ç»­è®­ç»ƒ\")\n",
    "print(\"4. quick_train(data_folder, target_snr, num_epochs) - å¿«é€Ÿè®­ç»ƒ\")\n",
    "print(\"5. list_available_checkpoints(checkpoint_manager) - æŸ¥çœ‹æ£€æŸ¥ç‚¹\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f832d",
   "metadata": {},
   "source": [
    "# Training from scratch\n",
    "First trained with SNR 200 data, then continue training with SNR 50, 100 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4834ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸåŠ è½½ 135649 ä¸ªæ–‡ä»¶\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# init the dataset\n",
    "data_folder = \"/home/ud202180035/DNN_for_GW_Parameter_Space_Estimation/waveforms2\"  # ä½ çš„æ•°æ®ç›®å½•ï¼Œå¯æŒ‰éœ€ä¿®æ”¹\n",
    "target_snr = [\"200.00\"]  # å¯æŒ‡å®šåŠ è½½å“ªäº› SNR æ•°æ®\n",
    "\n",
    "dataset = GWDataset(data_folder, target_snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b79842",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = split_dataset(dataset)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True,  num_workers=24, pin_memory=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=32, shuffle=False, num_workers=24, pin_memory=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=32, shuffle=False, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3866f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WaveUNetWithTransformer(\n",
       "  (encoder1): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder2): Sequential(\n",
       "    (0): Conv1d(16, 32, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder3): Sequential(\n",
       "    (0): Conv1d(32, 64, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder4): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (transformer_input_proj): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_output_proj): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "  (decoder1): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(128, 64, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder2): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(64, 32, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder3): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(32, 16, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder4): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(16, 1, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# åˆå§‹åŒ–model\n",
    "denoising_model = WaveUNetWithTransformer()\n",
    "denoising_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229cb714",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(denoising_model.parameters(), lr=1e-5, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa55101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "best_loss = train_model_with_checkpoints(\n",
    "    model=denoising_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=200,\n",
    "    checkpoint_manager=checkpoint_manager,\n",
    "    resume_from_checkpoint=None,\n",
    "    save_every_n_epochs=10,\n",
    "    training_config={\n",
    "        \"snr\": [\"200.00\"],\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"weight_decay\": 1e-6,\n",
    "        \"save_every_n_epochs\": 10,\n",
    "        \"number of epochs\": 200\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"âœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700a750",
   "metadata": {},
   "source": [
    "# Continue training from best_model or resume from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc20450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸåŠ è½½ 135649 ä¸ªæ–‡ä»¶\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# init the dataset\n",
    "data_folder = \"/home/ud202180035/DNN_for_GW_Parameter_Space_Estimation/waveforms2\"  # ä½ çš„æ•°æ®ç›®å½•ï¼Œå¯æŒ‰éœ€ä¿®æ”¹\n",
    "target_snr = [\"50\",\"100\"]  # å¯æŒ‡å®šåŠ è½½å“ªäº› SNR æ•°æ®\n",
    "\n",
    "dataset = GWDataset(data_folder, target_snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62059db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = split_dataset(dataset)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True,  num_workers=24, pin_memory=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=32, shuffle=False, num_workers=24, pin_memory=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=32, shuffle=False, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5482a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8720530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WaveUNetWithTransformer(\n",
       "  (encoder1): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder2): Sequential(\n",
       "    (0): Conv1d(16, 32, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder3): Sequential(\n",
       "    (0): Conv1d(32, 64, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder4): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (transformer_input_proj): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_output_proj): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "  (decoder1): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(128, 64, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder2): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(64, 32, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder3): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(32, 16, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder4): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(16, 1, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# åˆå§‹åŒ–model\n",
    "denoising_model = WaveUNetWithTransformer()\n",
    "denoising_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9243af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "checkpoint_manager.load_best_model(denoising_model,\"best_denoising_model.pt\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948abef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ–°çš„ä¼˜åŒ–å™¨ï¼ˆå¯ä»¥ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒï¼‰\n",
    "optimizer = torch.optim.Adam(denoising_model.parameters(), lr=1e-6, weight_decay=1e-6)  # æ›´å°çš„å­¦ä¹ ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43cb855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ud202180035/miniconda3/envs/dnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: model_checkpoints/checkpoint_epoch_40_20250710_164543.pt\n",
      "ğŸ†• ä»å¤´å¼€å§‹è®­ç»ƒ\n",
      "ğŸš€ å¼€å§‹è®­ç»ƒ:\n",
      "   - æ¨¡å‹: WaveUNetWithTransformer\n",
      "   - æ€»å‚æ•°é‡: 2,497,729\n",
      "   - å¯è®­ç»ƒå‚æ•°: 2,497,729\n",
      "   - è®¾å¤‡: cuda\n",
      "   - è®­ç»ƒè½®æ¬¡: 0 -> 60\n",
      "   - å­¦ä¹ ç‡: 1e-06\n",
      "  1/60: Train Loss = 26.278693, Val Loss = 2.468107, Output Std = 0.2427, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.468107)\n",
      "  ğŸ† Best model saved!\n",
      "  2/60: Train Loss = 2.291680, Val Loss = 2.198200, Output Std = 0.2472, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.198200)\n",
      "  ğŸ† Best model saved!\n",
      "  3/60: Train Loss = 2.197308, Val Loss = 2.170144, Output Std = 0.2492, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.170144)\n",
      "  ğŸ† Best model saved!\n",
      "  4/60: Train Loss = 2.179067, Val Loss = 2.157876, Output Std = 0.2483, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.157876)\n",
      "  ğŸ† Best model saved!\n",
      "  5/60: Train Loss = 2.168617, Val Loss = 2.156744, Output Std = 0.2521, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.156744)\n",
      "  ğŸ† Best model saved!\n",
      "  6/60: Train Loss = 2.162716, Val Loss = 2.144276, Output Std = 0.2510, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.144276)\n",
      "  ğŸ† Best model saved!\n",
      "  7/60: Train Loss = 2.154733, Val Loss = 2.136824, Output Std = 0.2504, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.136824)\n",
      "  ğŸ† Best model saved!\n",
      "  8/60: Train Loss = 2.148793, Val Loss = 2.131226, Output Std = 0.2478, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.131226)\n",
      "  ğŸ† Best model saved!\n",
      "  9/60: Train Loss = 2.141751, Val Loss = 2.125323, Output Std = 0.2497, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.125323)\n",
      "  ğŸ† Best model saved!\n",
      " 10/60: Train Loss = 2.139210, Val Loss = 2.122159, Output Std = 0.2510, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.122159)\n",
      "  ğŸ† Best model saved!\n",
      "âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: model_checkpoints/checkpoint_epoch_10_20250723_173335.pt\n",
      " 11/60: Train Loss = 2.131683, Val Loss = 2.116201, Output Std = 0.2507, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.116201)\n",
      "  ğŸ† Best model saved!\n",
      " 12/60: Train Loss = 2.125454, Val Loss = 2.109856, Output Std = 0.2500, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.109856)\n",
      "  ğŸ† Best model saved!\n",
      " 13/60: Train Loss = 2.120737, Val Loss = 2.103640, Output Std = 0.2472, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.103640)\n",
      "  ğŸ† Best model saved!\n",
      " 14/60: Train Loss = 2.114977, Val Loss = 2.098335, Output Std = 0.2473, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.098335)\n",
      "  ğŸ† Best model saved!\n",
      " 15/60: Train Loss = 2.108821, Val Loss = 2.093383, Output Std = 0.2491, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.093383)\n",
      "  ğŸ† Best model saved!\n",
      " 16/60: Train Loss = 2.104333, Val Loss = 2.090895, Output Std = 0.2496, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.090895)\n",
      "  ğŸ† Best model saved!\n",
      " 17/60: Train Loss = 2.099904, Val Loss = 2.085953, Output Std = 0.2471, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.085953)\n",
      "  ğŸ† Best model saved!\n",
      " 18/60: Train Loss = 2.097093, Val Loss = 2.084296, Output Std = 0.2491, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.084296)\n",
      "  ğŸ† Best model saved!\n",
      " 19/60: Train Loss = 2.094248, Val Loss = 2.080151, Output Std = 0.2479, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.080151)\n",
      "  ğŸ† Best model saved!\n",
      " 20/60: Train Loss = 2.091811, Val Loss = 2.078814, Output Std = 0.2481, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.078814)\n",
      "  ğŸ† Best model saved!\n",
      "âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: model_checkpoints/checkpoint_epoch_20_20250723_233219.pt\n",
      " 21/60: Train Loss = 2.090958, Val Loss = 2.076461, Output Std = 0.2471, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.076461)\n",
      "  ğŸ† Best model saved!\n",
      " 22/60: Train Loss = 2.088698, Val Loss = 2.076264, Output Std = 0.2463, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.076264)\n",
      "  ğŸ† Best model saved!\n",
      " 23/60: Train Loss = 2.086484, Val Loss = 2.075483, Output Std = 0.2490, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.075483)\n",
      "  ğŸ† Best model saved!\n",
      " 24/60: Train Loss = 2.084955, Val Loss = 2.073728, Output Std = 0.2464, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.073728)\n",
      "  ğŸ† Best model saved!\n",
      " 25/60: Train Loss = 2.084319, Val Loss = 2.071787, Output Std = 0.2477, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.071787)\n",
      "  ğŸ† Best model saved!\n",
      " 26/60: Train Loss = 2.082461, Val Loss = 2.077116, Output Std = 0.2505, LR = 1.00e-06\n",
      " 27/60: Train Loss = 2.082282, Val Loss = 2.070191, Output Std = 0.2471, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.070191)\n",
      "  ğŸ† Best model saved!\n",
      " 28/60: Train Loss = 2.081075, Val Loss = 2.069242, Output Std = 0.2482, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.069242)\n",
      "  ğŸ† Best model saved!\n",
      " 29/60: Train Loss = 2.081207, Val Loss = 2.068537, Output Std = 0.2484, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.068537)\n",
      "  ğŸ† Best model saved!\n",
      " 30/60: Train Loss = 2.079419, Val Loss = 2.071894, Output Std = 0.2501, LR = 1.00e-06\n",
      "âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: model_checkpoints/checkpoint_epoch_30_20250724_052846.pt\n",
      " 31/60: Train Loss = 2.077845, Val Loss = 2.068021, Output Std = 0.2487, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.068021)\n",
      "  ğŸ† Best model saved!\n",
      " 32/60: Train Loss = 2.078468, Val Loss = 2.067798, Output Std = 0.2492, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.067798)\n",
      "  ğŸ† Best model saved!\n",
      " 33/60: Train Loss = 2.077842, Val Loss = 2.067637, Output Std = 0.2487, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.067637)\n",
      "  ğŸ† Best model saved!\n",
      " 34/60: Train Loss = 2.077693, Val Loss = 2.067030, Output Std = 0.2493, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.067030)\n",
      "  ğŸ† Best model saved!\n",
      " 35/60: Train Loss = 2.076178, Val Loss = 2.065321, Output Std = 0.2476, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.065321)\n",
      "  ğŸ† Best model saved!\n",
      " 36/60: Train Loss = 2.075812, Val Loss = 2.064727, Output Std = 0.2475, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.064727)\n",
      "  ğŸ† Best model saved!\n",
      " 37/60: Train Loss = 2.075676, Val Loss = 2.064232, Output Std = 0.2478, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.064232)\n",
      "  ğŸ† Best model saved!\n",
      " 38/60: Train Loss = 2.075652, Val Loss = 2.064529, Output Std = 0.2490, LR = 1.00e-06\n",
      " 39/60: Train Loss = 2.074099, Val Loss = 2.063961, Output Std = 0.2474, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.063961)\n",
      "  ğŸ† Best model saved!\n",
      " 40/60: Train Loss = 2.073581, Val Loss = 2.063130, Output Std = 0.2476, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.063130)\n",
      "  ğŸ† Best model saved!\n",
      "âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: model_checkpoints/checkpoint_epoch_40_20250724_112444.pt\n",
      " 41/60: Train Loss = 2.073601, Val Loss = 2.062955, Output Std = 0.2473, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.062955)\n",
      "  ğŸ† Best model saved!\n",
      " 42/60: Train Loss = 2.073411, Val Loss = 2.062614, Output Std = 0.2486, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.062614)\n",
      "  ğŸ† Best model saved!\n",
      " 43/60: Train Loss = 2.073151, Val Loss = 2.062433, Output Std = 0.2486, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.062433)\n",
      "  ğŸ† Best model saved!\n",
      " 44/60: Train Loss = 2.072010, Val Loss = 2.061690, Output Std = 0.2482, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.061690)\n",
      "  ğŸ† Best model saved!\n",
      " 45/60: Train Loss = 2.072743, Val Loss = 2.061618, Output Std = 0.2486, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.061618)\n",
      "  ğŸ† Best model saved!\n",
      " 46/60: Train Loss = 2.072353, Val Loss = 2.061198, Output Std = 0.2475, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.061198)\n",
      "  ğŸ† Best model saved!\n",
      " 47/60: Train Loss = 2.071891, Val Loss = 2.061449, Output Std = 0.2489, LR = 1.00e-06\n",
      " 48/60: Train Loss = 2.071164, Val Loss = 2.061219, Output Std = 0.2478, LR = 1.00e-06\n",
      " 49/60: Train Loss = 2.071487, Val Loss = 2.061932, Output Std = 0.2494, LR = 1.00e-06\n",
      " 50/60: Train Loss = 2.071574, Val Loss = 2.061373, Output Std = 0.2493, LR = 1.00e-06\n",
      "âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: model_checkpoints/checkpoint_epoch_50_20250724_172205.pt\n",
      "ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ£€æŸ¥ç‚¹: checkpoint_epoch_10_20250723_173335.pt\n",
      "ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ£€æŸ¥ç‚¹: checkpoint_epoch_20_20250723_233219.pt\n",
      "ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ£€æŸ¥ç‚¹: checkpoint_epoch_30_20250724_052846.pt\n",
      "ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ£€æŸ¥ç‚¹: checkpoint_epoch_40_20250724_112444.pt\n",
      "ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ£€æŸ¥ç‚¹: checkpoint_epoch_50_20250724_172205.pt\n",
      "ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ£€æŸ¥ç‚¹: checkpoint_epoch_60_20250711_043605.pt\n",
      "ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ£€æŸ¥ç‚¹: checkpoint_epoch_70_20250711_103159.pt\n",
      "âœ… æ¸…ç†å®Œæˆï¼Œä¿ç•™äº†æœ€è¿‘çš„ 5 ä¸ªæ£€æŸ¥ç‚¹\n",
      " 51/60: Train Loss = 2.070379, Val Loss = 2.060905, Output Std = 0.2491, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.060905)\n",
      "  ğŸ† Best model saved!\n",
      " 52/60: Train Loss = 2.070767, Val Loss = 2.059791, Output Std = 0.2481, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.059791)\n",
      "  ğŸ† Best model saved!\n",
      " 53/60: Train Loss = 2.070148, Val Loss = 2.060374, Output Std = 0.2472, LR = 1.00e-06\n",
      " 54/60: Train Loss = 2.070471, Val Loss = 2.059828, Output Std = 0.2488, LR = 1.00e-06\n",
      " 55/60: Train Loss = 2.069950, Val Loss = 2.064241, Output Std = 0.2505, LR = 1.00e-06\n",
      " 56/60: Train Loss = 2.070581, Val Loss = 2.059057, Output Std = 0.2479, LR = 1.00e-06\n",
      "ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: model_checkpoints/best_denoising_model.pt (éªŒè¯æŸå¤±: 2.059057)\n",
      "  ğŸ† Best model saved!\n",
      " 57/60: Train Loss = 2.070071, Val Loss = 2.059583, Output Std = 0.2489, LR = 1.00e-06\n",
      " 58/60: Train Loss = 2.069849, Val Loss = 2.059489, Output Std = 0.2491, LR = 1.00e-06\n",
      " 59/60: Train Loss = 2.068840, Val Loss = 2.059451, Output Std = 0.2471, LR = 1.00e-06\n",
      " 60/60: Train Loss = 2.069535, Val Loss = 2.061387, Output Std = 0.2499, LR = 1.00e-06\n",
      "âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: model_checkpoints/checkpoint_epoch_60_20250724_232348.pt\n",
      "âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: model_checkpoints/final_checkpoint_epoch_60.pt\n",
      "ğŸ¯ è®­ç»ƒå®Œæˆ! æœ€ç»ˆæ£€æŸ¥ç‚¹: model_checkpoints/final_checkpoint_epoch_60.pt\n",
      "âœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: 2.059057\n"
     ]
    }
   ],
   "source": [
    "# ç»§ç»­è®­ç»ƒ\n",
    "best_loss = train_model_with_checkpoints(\n",
    "            model=denoising_model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            num_epochs=60,  # å¾®è°ƒè½®æ¬¡\n",
    "            checkpoint_manager=checkpoint_manager,\n",
    "            save_every_n_epochs=10,\n",
    "            training_config={\n",
    "                'fine_tuning': True,\n",
    "                'original_data': \"waveforms2\",\n",
    "                'new_data': \"waveforms2\",\n",
    "                'new_snr': [\"50\",\"100\"],\n",
    "                'learning_rate': optimizer.param_groups[0]['lr']\n",
    "            }\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4gw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
