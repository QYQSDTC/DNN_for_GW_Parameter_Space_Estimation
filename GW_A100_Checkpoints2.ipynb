{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b085485",
   "metadata": {},
   "source": [
    "# Training on A100 with checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4bfe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import platform\n",
    "\n",
    "# 根据操作系统设置中文字体\n",
    "system = platform.system()\n",
    "\n",
    "if system == \"Darwin\":  # macOS\n",
    "    plt.rcParams['font.sans-serif'] = ['PingFang SC', 'Heiti SC', 'STHeiti', 'Arial Unicode MS']\n",
    "elif system == \"Linux\":\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'WenQuanYi Micro Hei', 'Noto Sans CJK SC']\n",
    "elif system == \"Windows\":\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'KaiTi', 'FangSong']\n",
    "else:\n",
    "    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "\n",
    "# 解决负号显示问题\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"已设置{system}系统的中文字体支持\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc184a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 初始化自定义 Dataset ==========\n",
    "class GWDataset(Dataset):\n",
    "    def __init__(self, folder_path, snr_list=None, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        self.snr_list = snr_list or [\"50.00\", \"100.00\", \"200.00\"]\n",
    "        \n",
    "        # 检查文件夹是否存在\n",
    "        if not os.path.isdir(folder_path):\n",
    "            raise FileNotFoundError(f\"数据目录不存在: {folder_path}\")\n",
    "            \n",
    "        self.file_index = []\n",
    "        snr_pattern = re.compile(r\"_SNR(\\d+\\.\\d+)\\.h5\")\n",
    "\n",
    "        # 使用更安全的方式遍历文件\n",
    "        for fname in os.listdir(folder_path):\n",
    "            if not fname.endswith(\".h5\"):\n",
    "                continue\n",
    "                \n",
    "            full_path = os.path.join(folder_path, fname)\n",
    "            \n",
    "            # 跳过非文件项（如目录）\n",
    "            if not os.path.isfile(full_path):\n",
    "                print(f\"跳过非文件项: {full_path}\")\n",
    "                continue\n",
    "                \n",
    "            # 检查文件可读性\n",
    "            if not os.access(full_path, os.R_OK):\n",
    "                print(f\"警告: 文件不可读，跳过: {full_path}\")\n",
    "                continue\n",
    "                \n",
    "            match = snr_pattern.search(fname)\n",
    "            if match:\n",
    "                snr = match.group(1)\n",
    "                if snr in self.snr_list:\n",
    "                    # 尝试打开文件以验证完整性\n",
    "                    try:\n",
    "                        with h5py.File(full_path, \"r\") as f:\n",
    "                            # 简单验证文件结构\n",
    "                            if \"Data\" not in f:\n",
    "                                print(f\"警告: 文件缺少'Data'组，跳过: {full_path}\")\n",
    "                                continue\n",
    "                                \n",
    "                            # 检查必要属性\n",
    "                            data_group = f[\"Data\"]\n",
    "                            required_attrs = [\"mc_true\", \"phis_true\", \"thetas_true\"]\n",
    "                            if not all(attr in data_group.attrs for attr in required_attrs):\n",
    "                                print(f\"警告: 文件缺少必要属性，跳过: {full_path}\")\n",
    "                                continue\n",
    "                                \n",
    "                        # 文件验证通过，添加到索引\n",
    "                        self.file_index.append((full_path, snr))\n",
    "                        \n",
    "                    except (OSError, IOError) as e:\n",
    "                        print(f\"文件打开错误 {full_path}: {str(e)}，跳过\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"处理文件时出错 {full_path}: {str(e)}，跳过\")\n",
    "\n",
    "        if not self.file_index:\n",
    "            raise ValueError(\"没有匹配到任何指定 SNR 的文件，请检查路径或 snr_list 设置\")\n",
    "        else:\n",
    "            print(f\"成功加载 {len(self.file_index)} 个文件\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, snr = self.file_index[idx]\n",
    "        \n",
    "        try:\n",
    "            with h5py.File(file_path, \"r\") as f:\n",
    "                data_group = f[\"Data\"]\n",
    "                \n",
    "                # 处理不同的数据结构\n",
    "                if isinstance(data_group, h5py.Group):\n",
    "                    white_data = torch.tensor(data_group[\"white_Data\"][:], dtype=torch.float32)\n",
    "                    white_signal = torch.tensor(data_group[\"white_signal\"][:], dtype=torch.float32)\n",
    "                elif isinstance(data_group, h5py.Dataset):\n",
    "                    # 处理旧格式的数据集\n",
    "                    white_data = torch.tensor(data_group[0].flatten(), dtype=torch.float32)\n",
    "                    white_signal = torch.tensor(data_group[1].flatten(), dtype=torch.float32)\n",
    "                else:\n",
    "                    raise ValueError(f\"未知的数据结构: {file_path}\")\n",
    "                \n",
    "                attrs = {k: data_group.attrs[k] for k in data_group.attrs}\n",
    "                \n",
    "        except (OSError, IOError) as e:\n",
    "            # 文件读取错误时返回空数据并记录警告\n",
    "            print(f\"读取文件错误 {file_path}: {str(e)}\")\n",
    "            seq_len = 6184  # 默认序列长度\n",
    "            white_data = torch.zeros(seq_len, dtype=torch.float32)\n",
    "            white_signal = torch.zeros(seq_len, dtype=torch.float32)\n",
    "            attrs = {\n",
    "                \"mc_true\": 0.0,\n",
    "                \"phis_true\": 0.0,\n",
    "                \"thetas_true\": 0.0\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"处理文件时发生意外错误 {file_path}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "        sample = {\n",
    "            \"white_data\": white_data,\n",
    "            \"white_signal\": white_signal,\n",
    "            \"attributes\": attrs,\n",
    "            \"mc_true\": attrs.get(\"mc_true\", 0.0),\n",
    "            \"phis_true\": attrs.get(\"phis_true\", 0.0),\n",
    "            \"thetas_true\": attrs.get(\"thetas_true\", 0.0),\n",
    "            \"snr\": snr,\n",
    "            \"filename\": os.path.basename(file_path)\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009daab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建掩码 1表示弱信号，0表示强信号，2表示第二段弱信号\n",
    "def generate_mask(data: torch.Tensor, threshold_factor=5.0):\n",
    "    B, L = data.shape\n",
    "    mask = torch.ones_like(data, dtype=torch.long)  # 初始为全1（弱信号）\n",
    "    stds = torch.std(data, dim=1, keepdim=True)  # (B, 1)\n",
    "    thresholds = threshold_factor * stds         # 每条数据的阈值 (B, 1)\n",
    "\n",
    "    abs_data = data.abs()  # (B, L)\n",
    "    for i in range(B):\n",
    "        above_th = (abs_data[i] > thresholds[i])  # bool mask\n",
    "        strong_indices = torch.nonzero(above_th).squeeze()\n",
    "\n",
    "        if strong_indices.numel() > 0:\n",
    "            # 处理0维张量的情况（只有一个强信号点）\n",
    "            if strong_indices.dim() == 0:\n",
    "                start = end = strong_indices.item()\n",
    "            else:\n",
    "                start = strong_indices[0].item()\n",
    "                end = strong_indices[-1].item()\n",
    "            \n",
    "            mask[i, start:end+1] = 0  # 强信号（包含end点）\n",
    "            mask[i, end+1:] = 2       # 第二段弱信号\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e9d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "def standardize_batch(data: torch.Tensor, signal: torch.Tensor, mask: torch.Tensor, amplification=1.0):\n",
    "    # 放大弱信号部分\n",
    "    signal_amplified = signal.clone()\n",
    "    signal_amplified[mask == 1] *= amplification\n",
    "\n",
    "    # 提取弱信号索引\n",
    "    weak_indices = (mask == 1)\n",
    "\n",
    "    # RobustScaler 模拟：中位数与 IQR（近似标准化）\n",
    "    weak_values = data[weak_indices].view(-1)\n",
    "    median = weak_values.median()\n",
    "    q1 = weak_values.kthvalue(int(len(weak_values) * 0.25))[0]\n",
    "    q3 = weak_values.kthvalue(int(len(weak_values) * 0.75))[0]\n",
    "    iqr = q3 - q1 + 1e-8  # 避免除零\n",
    "\n",
    "    # 标准化公式：(x - median) / IQR\n",
    "    data_std = (data - median) / iqr\n",
    "    signal_std = (signal_amplified - median) / iqr\n",
    "\n",
    "    stats = {\"median\": median.item(), \"iqr\": iqr.item()}\n",
    "    return data_std, signal_std, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e4401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集划分（训练/验证/测试）\n",
    "def split_dataset(dataset, train_ratio=0.2, val_ratio=0.2):\n",
    "    total_len = len(dataset)\n",
    "    train_len = int(total_len * train_ratio)\n",
    "    val_len = int(total_len * val_ratio)\n",
    "    test_len = total_len - train_len - val_len\n",
    "    return random_split(dataset, [train_len, val_len, test_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=7, stride=stride, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.LeakyReLU = nn.LeakyReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.LeakyReLU(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.LeakyReLU(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.LeakyReLU = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self.make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, 1)  # 输出一个值\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.LeakyReLU(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNetModel():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce33c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveUNetWithTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder（使用大卷积核，并增加层数）\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=15, padding=7), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=15, padding=7), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=15, padding=7), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=15, padding=7), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        # Transformer bottleneck\n",
    "        self.transformer_input_proj = nn.Conv1d(128, 256, kernel_size=1)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=512, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        self.transformer_output_proj = nn.Conv1d(256, 128, kernel_size=1)\n",
    "\n",
    "        # Decoder（镜像结构）\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True),\n",
    "            nn.Conv1d(128, 64, kernel_size=15, padding=7), nn.ReLU()\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True),\n",
    "            nn.Conv1d(64, 32, kernel_size=15, padding=7), nn.ReLU()\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True),\n",
    "            nn.Conv1d(32, 16, kernel_size=15, padding=7), nn.ReLU()\n",
    "        )\n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True),\n",
    "            nn.Conv1d(16, 1, kernel_size=15, padding=7)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        input_len = x.shape[-1]\n",
    "        residual = x  # 用于强信号跳跃连接\n",
    "\n",
    "        x1 = self.encoder1(x)\n",
    "        x2 = self.encoder2(x1)\n",
    "        x3 = self.encoder3(x2)\n",
    "        x4 = self.encoder4(x3)\n",
    "\n",
    "        x_trans = self.transformer_input_proj(x4).permute(0, 2, 1)\n",
    "        x_trans = self.transformer(x_trans)\n",
    "        x_trans = self.transformer_output_proj(x_trans.permute(0, 2, 1))\n",
    "\n",
    "        x = self.decoder1(x_trans)\n",
    "        x = self.decoder2(x)\n",
    "        x = self.decoder3(x)\n",
    "        x = self.decoder4(x)\n",
    "\n",
    "        # 输出与输入对齐\n",
    "        if x.shape[-1] > input_len:\n",
    "            x = x[:, :, :input_len]\n",
    "        elif x.shape[-1] < input_len:\n",
    "            x = F.pad(x, (0, input_len - x.shape[-1]))\n",
    "        \n",
    "        # 输出处理：根据掩码合成最终输出\n",
    "        # output = x * (mask == 1) + residual * (mask == 0)  # 弱信号①+强信号\n",
    "        # output = output * (mask != 2)  # 再将弱信号②置 0\n",
    "        output = (x * (mask == 1).float() + residual * (mask == 0).float()) * (mask != 2).float()\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28dd927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 掩码损失函数\n",
    "def masked_loss(output, target, mask, lambda_mse=5.0, lambda_smooth=1.0, stability_penalty=1.0):\n",
    "    # ========== 主损失项 (MSE) ==========\n",
    "    loss = F.mse_loss(output, target, reduction='none')  # (B, 1, T)\n",
    "    active = (mask == 1).float()\n",
    "    core = (loss * active).sum() / (active.sum() + 1e-8)\n",
    "    # ========== 平滑项（仅对弱信号①） ==========\n",
    "    diff = output[:, :, 1:] - output[:, :, :-1]\n",
    "    mask_diff = (mask[:, :, 1:] == 1) & (mask[:, :, :-1] == 1)  # 相邻都是弱信号①\n",
    "    smooth_penalty = (diff**2 * mask_diff.float()).sum() / (mask_diff.float().sum() + 1e-8)\n",
    "    # ========== 标准差惩罚项（防塌缩，仅对弱信号①） ==========\n",
    "    weak_output = output[mask == 1]\n",
    "    std_penalty = 1.0 / (torch.std(weak_output) + 1e-4)  # 防止输出塌缩为常数\n",
    "    # ========== 总损失 ==========\n",
    "    total_loss = lambda_mse * core + lambda_smooth * smooth_penalty + stability_penalty * std_penalty\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# denoising def selective_inverse_transform(signal, mask, stats, amplification=10.0):\n",
    "    signal = signal.copy()\n",
    "    signal = signal * stats[\"iqr\"] + stats[\"median\"]  # 所有区域反标准化\n",
    "    signal[mask == 1] /= amplification                # 仅弱信号再除以放大倍数\n",
    "    return signal\n",
    "\n",
    "# 可视化去噪效果（输入、预测、纯信号）\n",
    "def visualize_denoising_subplots(model, test_loader, device, sample_index=0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(test_loader))\n",
    "        white_data = batch[\"white_data\"].to(device)\n",
    "        white_signal = batch[\"white_signal\"].to(device)\n",
    "        mask = generate_mask(white_data).unsqueeze(1).to(device)\n",
    "        print(mask[sample_index])\n",
    "\n",
    "        x = white_data.unsqueeze(1)\n",
    "        y = white_signal.unsqueeze(1)\n",
    "\n",
    "        # 标准化 + 获取统计量\n",
    "        x_std, y_std, stats = standardize_batch(x.squeeze(1), y.squeeze(1), mask.squeeze(1))\n",
    "        x_std = x_std.unsqueeze(1).to(device)\n",
    "        y_std = y_std.unsqueeze(1).to(device)\n",
    "\n",
    "        output = model(x_std, mask).squeeze(1).cpu().numpy()\n",
    "        print(output[sample_index])\n",
    "\n",
    "    # 选择样本\n",
    "    input_signal = x[sample_index].squeeze().cpu().numpy()\n",
    "    denoised_signal = output[sample_index]\n",
    "    clean_signal = y[sample_index].squeeze().cpu().numpy()\n",
    "    signal_mask = mask[sample_index].squeeze().cpu().numpy()\n",
    "\n",
    "    # 反归一化\n",
    "    input_signal = selective_inverse_transform(input_signal, signal_mask, stats, amplification=1.0)\n",
    "    denoised_signal = selective_inverse_transform(denoised_signal, signal_mask, stats, amplification=1.0)\n",
    "    clean_signal = selective_inverse_transform(clean_signal, signal_mask, stats, amplification=1.0)\n",
    "\n",
    "    # 绘图\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(15, 9), sharex=True)\n",
    "    axs[0].plot(input_signal, color='orange')\n",
    "    axs[0].set_title(\"Noisy Input\")\n",
    "    axs[1].plot(denoised_signal, color='green')\n",
    "    axs[1].set_title(\"Denoised Output\")\n",
    "    axs[2].plot(clean_signal, color='blue')\n",
    "    axs[2].set_title(\"Ground Truth Signal\")\n",
    "\n",
    "    for ax in axs:\n",
    "        # ax.set_ylim(-1, 1)\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def test_model_and_save_labels_batched(model, test_loader, device, output_dir=\"pt_chunks\", base_name=\"denoised_batch\", target_names=[\"mc_true\", \"phis_true\", \"thetas_true\"]):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            white_data = batch[\"white_data\"].to(device)\n",
    "            mask = generate_mask(white_data).unsqueeze(1).to(device)\n",
    "            x = white_data.unsqueeze(1)\n",
    "\n",
    "            # 标准化处理\n",
    "            x_std, _, _ = standardize_batch(x.squeeze(1), x.squeeze(1), mask.squeeze(1))\n",
    "            x_std = x_std.unsqueeze(1).to(device)\n",
    "\n",
    "            # 去噪输出\n",
    "            output = model(x_std, mask).cpu()\n",
    "\n",
    "            # 提取三类标签并组合为 (B, 3)\n",
    "            mc = torch.tensor(batch[\"mc_true\"], dtype=torch.float32).view(-1, 1)\n",
    "            phis = torch.tensor(batch[\"phis_true\"], dtype=torch.float32).view(-1, 1)\n",
    "            thetas = torch.tensor(batch[\"thetas_true\"], dtype=torch.float32).view(-1, 1)\n",
    "            targets = torch.cat([mc, phis, thetas], dim=1)\n",
    "\n",
    "            # 保存文件\n",
    "            file_path = os.path.join(output_dir, f\"{base_name}_{batch_count:04d}.pt\")\n",
    "            torch.save({\n",
    "                \"denoised\": output,\n",
    "                \"targets\": targets,\n",
    "                \"target_names\": target_names\n",
    "            }, file_path)\n",
    "            \n",
    "            torch.cuda.empty_cache()  # 保存后立即释放缓存\n",
    "\n",
    "            print(f\"[Saved] {file_path} ← {output.shape[0]} samples\")\n",
    "            batch_count += 1\n",
    "\n",
    "    print(f\"[Done] 共保存 {batch_count} 个批次文件于: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd1e3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型检查点管理器已初始化\n"
     ]
    }
   ],
   "source": [
    "## 完整的模型保存和加载系统\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"完整的模型检查点保存和加载系统\"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir=\"checkpoints\"):\n",
    "        \"\"\"\n",
    "        初始化模型检查点管理器\n",
    "        Args:\n",
    "            save_dir: 保存检查点的目录\n",
    "        \"\"\"\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    def save_checkpoint(self, model, optimizer, scheduler, epoch, train_loss, val_loss, \n",
    "                       best_val_loss, model_config=None, training_config=None, \n",
    "                       checkpoint_name=None):\n",
    "        \"\"\"\n",
    "        保存完整的模型检查点\n",
    "        Args:\n",
    "            model: 模型实例\n",
    "            optimizer: 优化器\n",
    "            scheduler: 学习率调度器\n",
    "            epoch: 当前轮次\n",
    "            train_loss: 训练损失\n",
    "            val_loss: 验证损失\n",
    "            best_val_loss: 最佳验证损失\n",
    "            model_config: 模型配置字典\n",
    "            training_config: 训练配置字典\n",
    "            checkpoint_name: 检查点名称，如果为None则自动生成\n",
    "        \"\"\"\n",
    "        if checkpoint_name is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            checkpoint_name = f\"checkpoint_epoch_{epoch}_{timestamp}.pt\"\n",
    "        \n",
    "        checkpoint_path = os.path.join(self.save_dir, checkpoint_name)\n",
    "        \n",
    "        # 准备保存的数据\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_config': model_config or {},\n",
    "            'training_config': training_config or {}\n",
    "        }\n",
    "        \n",
    "        # 保存检查点\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        # 保存元数据到JSON文件（便于查看）\n",
    "        metadata = {\n",
    "            'checkpoint_name': checkpoint_name,\n",
    "            'epoch': epoch,\n",
    "            'train_loss': float(train_loss),\n",
    "            'val_loss': float(val_loss),\n",
    "            'best_val_loss': float(best_val_loss),\n",
    "            'timestamp': checkpoint['timestamp'],\n",
    "            'model_config': model_config or {},\n",
    "            'training_config': training_config or {}\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(self.save_dir, checkpoint_name.replace('.pt', '_metadata.json'))\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"✅ 检查点已保存: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path, model, optimizer=None, scheduler=None, device='cpu'):\n",
    "        \"\"\"\n",
    "        加载模型检查点\n",
    "        Args:\n",
    "            checkpoint_path: 检查点文件路径\n",
    "            model: 模型实例\n",
    "            optimizer: 优化器（可选）\n",
    "            scheduler: 学习率调度器（可选）\n",
    "            device: 设备\n",
    "        Returns:\n",
    "            dict: 包含加载信息的字典\n",
    "        \"\"\"\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"检查点文件不存在: {checkpoint_path}\")\n",
    "        \n",
    "        print(f\"📂 正在加载检查点: {checkpoint_path}\")\n",
    "        \n",
    "        # 加载检查点\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        \n",
    "        # 加载模型状态\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # 加载优化器状态（如果提供）\n",
    "        if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(\"✅ 优化器状态已加载\")\n",
    "        \n",
    "        # 加载调度器状态（如果提供）\n",
    "        if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            print(\"✅ 学习率调度器状态已加载\")\n",
    "        \n",
    "        # 提取训练信息\n",
    "        loaded_info = {\n",
    "            'epoch': checkpoint.get('epoch', 0),\n",
    "            'train_loss': checkpoint.get('train_loss', float('inf')),\n",
    "            'val_loss': checkpoint.get('val_loss', float('inf')),\n",
    "            'best_val_loss': checkpoint.get('best_val_loss', float('inf')),\n",
    "            'timestamp': checkpoint.get('timestamp', 'unknown'),\n",
    "            'model_config': checkpoint.get('model_config', {}),\n",
    "            'training_config': checkpoint.get('training_config', {})\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ 检查点加载完成!\")\n",
    "        print(f\"   - 轮次: {loaded_info['epoch']}\")\n",
    "        print(f\"   - 验证损失: {loaded_info['val_loss']:.6f}\")\n",
    "        print(f\"   - 最佳验证损失: {loaded_info['best_val_loss']:.6f}\")\n",
    "        print(f\"   - 保存时间: {loaded_info['timestamp']}\")\n",
    "        \n",
    "        return loaded_info\n",
    "    \n",
    "    def save_best_model(self, model, val_loss, model_name=\"best_model.pt\"):\n",
    "        \"\"\"\n",
    "        保存最佳模型（只保存模型权重）\n",
    "        Args:\n",
    "            model: 模型实例\n",
    "            val_loss: 验证损失\n",
    "            model_name: 模型文件名\n",
    "        \"\"\"\n",
    "        model_path = os.path.join(self.save_dir, model_name)\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }, model_path)\n",
    "        print(f\"🏆 最佳模型已保存: {model_path} (验证损失: {val_loss:.6f})\")\n",
    "        return model_path\n",
    "    \n",
    "    def load_best_model(self, model, model_name=\"best_model.pt\", device='cpu'):\n",
    "        \"\"\"\n",
    "        加载最佳模型\n",
    "        Args:\n",
    "            model: 模型实例\n",
    "            model_name: 模型文件名\n",
    "            device: 设备\n",
    "        \"\"\"\n",
    "        model_path = os.path.join(self.save_dir, model_name)\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"最佳模型文件不存在: {model_path}\")\n",
    "        \n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        print(f\"🏆 最佳模型已加载: {model_path}\")\n",
    "        if 'val_loss' in checkpoint:\n",
    "            print(f\"   - 验证损失: {checkpoint['val_loss']:.6f}\")\n",
    "        if 'timestamp' in checkpoint:\n",
    "            print(f\"   - 保存时间: {checkpoint['timestamp']}\")\n",
    "    \n",
    "    def list_checkpoints(self):\n",
    "        \"\"\"列出所有可用的检查点\"\"\"\n",
    "        checkpoints = []\n",
    "        for file in os.listdir(self.save_dir):\n",
    "            if file.endswith('.pt') and not file.startswith('best_'):\n",
    "                metadata_file = file.replace('.pt', '_metadata.json')\n",
    "                metadata_path = os.path.join(self.save_dir, metadata_file)\n",
    "                \n",
    "                if os.path.exists(metadata_path):\n",
    "                    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "                        metadata = json.load(f)\n",
    "                    checkpoints.append(metadata)\n",
    "                else:\n",
    "                    # 如果没有元数据文件，只显示文件名\n",
    "                    checkpoints.append({'checkpoint_name': file, 'epoch': 'unknown'})\n",
    "        \n",
    "        # 按轮次排序\n",
    "        checkpoints.sort(key=lambda x: x.get('epoch', 0))\n",
    "        return checkpoints\n",
    "    \n",
    "    def cleanup_old_checkpoints(self, keep_last_n=5):\n",
    "        \"\"\"\n",
    "        清理旧的检查点，只保留最近的N个\n",
    "        Args:\n",
    "            keep_last_n: 保留的检查点数量\n",
    "        \"\"\"\n",
    "        checkpoints = self.list_checkpoints()\n",
    "        if len(checkpoints) <= keep_last_n:\n",
    "            print(f\"当前检查点数量({len(checkpoints)})未超过保留数量({keep_last_n})，无需清理\")\n",
    "            return\n",
    "        \n",
    "        # 删除旧的检查点\n",
    "        to_remove = checkpoints[:-keep_last_n]\n",
    "        for checkpoint in to_remove:\n",
    "            checkpoint_name = checkpoint['checkpoint_name']\n",
    "            checkpoint_path = os.path.join(self.save_dir, checkpoint_name)\n",
    "            metadata_path = checkpoint_path.replace('.pt', '_metadata.json')\n",
    "            \n",
    "            # 删除检查点文件\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                os.remove(checkpoint_path)\n",
    "                print(f\"🗑️ 已删除旧检查点: {checkpoint_name}\")\n",
    "            \n",
    "            # 删除元数据文件\n",
    "            if os.path.exists(metadata_path):\n",
    "                os.remove(metadata_path)\n",
    "        \n",
    "        print(f\"✅ 清理完成，保留了最近的 {keep_last_n} 个检查点\")\n",
    "\n",
    "# 创建检查点管理器实例\n",
    "checkpoint_manager = ModelCheckpoint(save_dir=\"model_checkpoints\")\n",
    "print(\"✅ 模型检查点管理器已初始化\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改进的训练函数，支持从检查点继续训练\n",
    "def train_model_with_checkpoints(model, train_loader, val_loader, optimizer, device, \n",
    "                                num_epochs=120, checkpoint_manager=None, \n",
    "                                resume_from_checkpoint=None, save_every_n_epochs=10,\n",
    "                                model_config=None, training_config=None):\n",
    "    \"\"\"\n",
    "    带检查点支持的训练函数\n",
    "    Args:\n",
    "        model: 模型实例\n",
    "        train_loader: 训练数据加载器\n",
    "        val_loader: 验证数据加载器\n",
    "        optimizer: 优化器\n",
    "        device: 设备\n",
    "        num_epochs: 总训练轮次\n",
    "        checkpoint_manager: 检查点管理器\n",
    "        resume_from_checkpoint: 要恢复的检查点路径\n",
    "        save_every_n_epochs: 每隔多少轮次保存一次检查点\n",
    "        model_config: 模型配置\n",
    "        training_config: 训练配置\n",
    "    \"\"\"\n",
    "    \n",
    "    # 初始化变量\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "    \n",
    "    # 如果需要从检查点恢复\n",
    "    if resume_from_checkpoint and checkpoint_manager:\n",
    "        try:\n",
    "            loaded_info = checkpoint_manager.load_checkpoint(\n",
    "                resume_from_checkpoint, model, optimizer, scheduler, device\n",
    "            )\n",
    "            start_epoch = loaded_info['epoch']\n",
    "            best_val_loss = loaded_info['best_val_loss']\n",
    "            print(f\"🔄 从轮次 {start_epoch} 继续训练，当前最佳验证损失: {best_val_loss:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 加载检查点失败: {e}\")\n",
    "            print(\"🆕 从头开始训练\")\n",
    "            start_epoch = 0\n",
    "            best_val_loss = float('inf')\n",
    "    \n",
    "    # 准备训练和模型配置信息\n",
    "    if training_config is None:\n",
    "        training_config = {\n",
    "            'num_epochs': num_epochs,\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "            'optimizer': type(optimizer).__name__,\n",
    "            'scheduler': type(scheduler).__name__,\n",
    "            'device': str(device),\n",
    "            'batch_size': train_loader.batch_size,\n",
    "            'train_dataset_size': len(train_loader.dataset),\n",
    "            'val_dataset_size': len(val_loader.dataset)\n",
    "        }\n",
    "    \n",
    "    if model_config is None:\n",
    "        model_config = {\n",
    "            'model_type': type(model).__name__,\n",
    "            'total_params': sum(p.numel() for p in model.parameters()),\n",
    "            'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        }\n",
    "    \n",
    "    print(f\"🚀 开始训练:\")\n",
    "    print(f\"   - 模型: {model_config['model_type']}\")\n",
    "    print(f\"   - 总参数量: {model_config['total_params']:,}\")\n",
    "    print(f\"   - 可训练参数: {model_config['trainable_params']:,}\")\n",
    "    print(f\"   - 设备: {device}\")\n",
    "    print(f\"   - 训练轮次: {start_epoch} -> {num_epochs}\")\n",
    "    print(f\"   - 学习率: {training_config['learning_rate']}\")\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            white_data = batch[\"white_data\"].to(device)\n",
    "            white_signal = batch[\"white_signal\"].to(device)\n",
    "\n",
    "            mask = generate_mask(white_data).unsqueeze(1).to(device)\n",
    "\n",
    "            x = white_data.unsqueeze(1)\n",
    "            y = white_signal.unsqueeze(1)\n",
    "\n",
    "            x_std, y_std, _ = standardize_batch(x.squeeze(1), y.squeeze(1), mask.squeeze(1))\n",
    "            x_std = x_std.unsqueeze(1).to(device)\n",
    "            y_std = y_std.unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_std, mask)\n",
    "            loss = masked_loss(output, y_std, mask)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "        \n",
    "        train_loss /= train_batches\n",
    "\n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                white_data = batch[\"white_data\"].to(device)\n",
    "                white_signal = batch[\"white_signal\"].to(device)\n",
    "\n",
    "                mask = generate_mask(white_data).unsqueeze(1).to(device)\n",
    "\n",
    "                x = white_data.unsqueeze(1)\n",
    "                y = white_signal.unsqueeze(1)\n",
    "\n",
    "                x_std, y_std, _ = standardize_batch(x.squeeze(1), y.squeeze(1), mask.squeeze(1))\n",
    "                x_std = x_std.unsqueeze(1).to(device)\n",
    "                y_std = y_std.unsqueeze(1).to(device)\n",
    "\n",
    "                output = model(x_std, mask)\n",
    "                loss = masked_loss(output, y_std, mask)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        val_loss /= val_batches\n",
    "\n",
    "        # 学习率调度\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # 输出训练信息\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        output_std = output.std().item() if 'output' in locals() else 0.0\n",
    "        \n",
    "        print(f\"{epoch+1:3d}/{num_epochs}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}, \"\n",
    "              f\"Output Std = {output_std:.4f}, LR = {current_lr:.2e}\")\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if checkpoint_manager:\n",
    "                checkpoint_manager.save_best_model(model, val_loss, \"best_denoising_model.pt\")\n",
    "            else:\n",
    "                torch.save(model.state_dict(), \"best_denoising_model.pt\")\n",
    "            print(\"  🏆 Best model saved!\")\n",
    "\n",
    "        # 定期保存检查点\n",
    "        if checkpoint_manager and (epoch + 1) % save_every_n_epochs == 0:\n",
    "            checkpoint_manager.save_checkpoint(\n",
    "                model, optimizer, scheduler, epoch + 1, train_loss, val_loss, \n",
    "                best_val_loss, model_config, training_config\n",
    "            )\n",
    "        \n",
    "        # 每50轮清理一次旧检查点\n",
    "        if checkpoint_manager and (epoch + 1) % 50 == 0:\n",
    "            checkpoint_manager.cleanup_old_checkpoints(keep_last_n=5)\n",
    "    \n",
    "    # 训练结束后保存最终检查点\n",
    "    if checkpoint_manager:\n",
    "        final_checkpoint = checkpoint_manager.save_checkpoint(\n",
    "            model, optimizer, scheduler, num_epochs, train_loss, val_loss, \n",
    "            best_val_loss, model_config, training_config, \n",
    "            checkpoint_name=f\"final_checkpoint_epoch_{num_epochs}.pt\"\n",
    "        )\n",
    "        print(f\"🎯 训练完成! 最终检查点: {final_checkpoint}\")\n",
    "    \n",
    "    print(f\"✅ 训练完成! 最佳验证损失: {best_val_loss:.6f}\")\n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c25f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实用工具函数\n",
    "def list_available_checkpoints(checkpoint_manager):\n",
    "    \"\"\"列出所有可用的检查点\"\"\"\n",
    "    print(\"📋 可用的检查点:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    checkpoints = checkpoint_manager.list_checkpoints()\n",
    "    if not checkpoints:\n",
    "        print(\"   暂无检查点\")\n",
    "        return\n",
    "    \n",
    "    for i, checkpoint in enumerate(checkpoints):\n",
    "        print(f\"{i+1:2d}. {checkpoint['checkpoint_name']}\")\n",
    "        print(f\"     轮次: {checkpoint.get('epoch', 'unknown')}\")\n",
    "        print(f\"     验证损失: {checkpoint.get('val_loss', 'unknown')}\")\n",
    "        print(f\"     保存时间: {checkpoint.get('timestamp', 'unknown')}\")\n",
    "        print()\n",
    "\n",
    "def create_new_training_session(data_folder, target_snr=[\"200.00\"], model_type=\"WaveUNet\"):\n",
    "    \"\"\"创建新的训练会话\"\"\"\n",
    "    print(\"🆕 创建新的训练会话...\")\n",
    "    \n",
    "    # 重新加载数据集\n",
    "    dataset = GWDataset(data_folder, snr_list=target_snr)\n",
    "    print(f\"📊 数据集加载完成: {len(dataset)} 个样本\")\n",
    "    \n",
    "    # 数据集划分\n",
    "    train_set, val_set, test_set = split_dataset(dataset)\n",
    "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=24, pin_memory=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=24, pin_memory=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=32, shuffle=False, num_workers=24, pin_memory=True)\n",
    "    \n",
    "    # 创建模型\n",
    "    if model_type == \"WaveUNet\":\n",
    "        model = WaveUNetWithTransformer()\n",
    "    elif model_type == \"ResNet\":\n",
    "        model = ResNetModel()\n",
    "    else:\n",
    "        raise ValueError(f\"不支持的模型类型: {model_type}\")\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "    \n",
    "    return model, optimizer, train_loader, val_loader, test_loader\n",
    "\n",
    "def continue_training_from_checkpoint(checkpoint_path, data_folder=None, target_snr=None):\n",
    "    \"\"\"从检查点继续训练\"\"\"\n",
    "    print(\"🔄 从检查点继续训练...\")\n",
    "    \n",
    "    # 如果提供了新的数据，重新加载数据集\n",
    "    if data_folder and target_snr:\n",
    "        print(\"📊 使用新的训练数据...\")\n",
    "        model, optimizer, train_loader, val_loader, test_loader = create_new_training_session(\n",
    "            data_folder, target_snr\n",
    "        )\n",
    "    else:\n",
    "        print(\"📊 使用原有的训练数据...\")\n",
    "        # 这里应该使用之前的数据加载器，如果没有则需要重新创建\n",
    "        # 为了演示，我们重新创建\n",
    "        model, optimizer, train_loader, val_loader, test_loader = create_new_training_session(\n",
    "            \"/home/ud202180035/DNN_for_GW_Parameter_Space_Estimation/waveforms2\", [\"200.00\"]\n",
    "        )\n",
    "    \n",
    "    return model, optimizer, train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c9027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型保存和加载系统已准备就绪!\n",
      "\n",
      "📖 使用方法:\n",
      "1. example_train_from_scratch() - 从头开始训练\n",
      "2. example_continue_from_checkpoint() - 从检查点继续\n",
      "3. example_continue_with_new_data() - 更换数据继续训练\n",
      "4. quick_train(data_folder, target_snr, num_epochs) - 快速训练\n",
      "5. list_available_checkpoints(checkpoint_manager) - 查看检查点\n"
     ]
    }
   ],
   "source": [
    "## 使用示例和说明\n",
    "\n",
    "\"\"\"\n",
    "使用指南：模型保存和加载系统\n",
    "\n",
    "1. 从头开始训练：\n",
    "   - 创建新的训练会话\n",
    "   - 使用 train_model_with_checkpoints 进行训练\n",
    "   - 系统会自动保存最佳模型和定期检查点\n",
    "\n",
    "2. 从检查点继续训练：\n",
    "   - 查看可用检查点\n",
    "   - 选择要继续的检查点\n",
    "   - 可以使用相同数据或更换新数据继续训练\n",
    "\n",
    "3. 使用不同数据继续训练：\n",
    "   - 加载预训练模型\n",
    "   - 更换训练数据集\n",
    "   - 继续训练以适应新数据\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 示例 1: 从头开始训练\n",
    "# ============================================================================\n",
    "def example_train_from_scratch():\n",
    "    \"\"\"示例：从头开始训练\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"示例 1: 从头开始训练\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 创建新的训练会话\n",
    "    model, optimizer, train_loader, val_loader, test_loader = create_new_training_session(\n",
    "        data_folder=\"/home/ud202180035/DNN_for_GW_Parameter_Space_Estimation/waveforms2\",\n",
    "        target_snr=[\"200.00\"],\n",
    "        model_type=\"WaveUNet\"\n",
    "    )\n",
    "    \n",
    "    # 开始训练（带检查点支持）\n",
    "    best_loss = train_model_with_checkpoints(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=200,  # 可根据需要调整\n",
    "        checkpoint_manager=checkpoint_manager,\n",
    "        save_every_n_epochs=10  # 每10轮保存一次检查点\n",
    "    )\n",
    "    \n",
    "    return model, best_loss\n",
    "\n",
    "# ============================================================================\n",
    "# 示例 2: 从检查点继续训练（相同数据）\n",
    "# ============================================================================\n",
    "def example_continue_from_checkpoint():\n",
    "    \"\"\"示例：从检查点继续训练\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"示例 2: 从检查点继续训练\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 查看可用检查点\n",
    "    list_available_checkpoints(checkpoint_manager)\n",
    "    \n",
    "    # 选择一个检查点（这里使用示例路径，实际使用时需要替换）\n",
    "    checkpoint_path = \"model_checkpoints/checkpoint_epoch_50_20231201_123456.pt\"  # 示例路径\n",
    "    \n",
    "    # 如果检查点存在，从该点继续训练\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        # 创建模型和数据加载器\n",
    "        model, optimizer, train_loader, val_loader, test_loader = continue_training_from_checkpoint(\n",
    "            checkpoint_path\n",
    "        )\n",
    "        \n",
    "        # 继续训练\n",
    "        best_loss = train_model_with_checkpoints(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            num_epochs=100,  # 训练到第100轮\n",
    "            checkpoint_manager=checkpoint_manager,\n",
    "            resume_from_checkpoint=checkpoint_path,\n",
    "            save_every_n_epochs=10\n",
    "        )\n",
    "        \n",
    "        return model, best_loss\n",
    "    else:\n",
    "        print(f\"❌ 检查点文件不存在: {checkpoint_path}\")\n",
    "        return None, None\n",
    "\n",
    "# ============================================================================\n",
    "# 示例 3: 更换数据继续训练\n",
    "# ============================================================================\n",
    "def example_continue_with_new_data():\n",
    "    \"\"\"示例：使用新数据继续训练\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"示例 3: 更换数据继续训练\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 加载之前训练的最佳模型\n",
    "    model = WaveUNetWithTransformer()\n",
    "    model.to(device)\n",
    "    \n",
    "    try:\n",
    "        # 加载最佳模型权重\n",
    "        checkpoint_manager.load_best_model(model, \"best_denoising_model.pt\", device)\n",
    "        \n",
    "        # 使用新的数据集（示例：不同的SNR）\n",
    "        new_data_folder = \"/home/ud202180035/DNN_for_GW_Parameter_Space_Estimation/waveforms_new\"  # 新数据路径\n",
    "        new_target_snr = [\"100.00\", \"150.00\"]  # 新的SNR设置\n",
    "        \n",
    "        # 创建新的数据加载器\n",
    "        new_dataset = GWDataset(new_data_folder, snr_list=new_target_snr)\n",
    "        new_train_set, new_val_set, new_test_set = split_dataset(new_dataset)\n",
    "        \n",
    "        new_train_loader = DataLoader(new_train_set, batch_size=32, shuffle=True, num_workers=24, pin_memory=True)\n",
    "        new_val_loader = DataLoader(new_val_set, batch_size=32, shuffle=False, num_workers=24, pin_memory=True)\n",
    "        \n",
    "        # 创建新的优化器（可以使用较小的学习率进行微调）\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-6)  # 更小的学习率\n",
    "        \n",
    "        # 继续训练\n",
    "        best_loss = train_model_with_checkpoints(\n",
    "            model=model,\n",
    "            train_loader=new_train_loader,\n",
    "            val_loader=new_val_loader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            num_epochs=30,  # 微调轮次\n",
    "            checkpoint_manager=checkpoint_manager,\n",
    "            save_every_n_epochs=5,\n",
    "            training_config={\n",
    "                'fine_tuning': True,\n",
    "                'original_data': \"waveforms2\",\n",
    "                'new_data': \"waveforms_new\",\n",
    "                'new_snr': new_target_snr\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return model, best_loss\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ 无法加载预训练模型: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ============================================================================\n",
    "# 实际使用时的简化接口\n",
    "# ============================================================================\n",
    "def quick_train(data_folder, target_snr=[\"200.00\"], num_epochs=50, resume_checkpoint=None):\n",
    "    \"\"\"快速训练接口\"\"\"\n",
    "    print(\"🚀 快速训练模式\")\n",
    "    \n",
    "    # 创建训练会话\n",
    "    model, optimizer, train_loader, val_loader, test_loader = create_new_training_session(\n",
    "        data_folder, target_snr\n",
    "    )\n",
    "    \n",
    "    # 开始训练\n",
    "    best_loss = train_model_with_checkpoints(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        checkpoint_manager=checkpoint_manager,\n",
    "        resume_from_checkpoint=resume_checkpoint,\n",
    "        save_every_n_epochs=max(1, num_epochs // 5)  # 保存5次检查点\n",
    "    )\n",
    "    \n",
    "    return model, best_loss\n",
    "\n",
    "print(\"✅ 模型保存和加载系统已准备就绪!\")\n",
    "print(\"\\n📖 使用方法:\")\n",
    "print(\"1. example_train_from_scratch() - 从头开始训练\")\n",
    "print(\"2. example_continue_from_checkpoint() - 从检查点继续\")\n",
    "print(\"3. example_continue_with_new_data() - 更换数据继续训练\")\n",
    "print(\"4. quick_train(data_folder, target_snr, num_epochs) - 快速训练\")\n",
    "print(\"5. list_available_checkpoints(checkpoint_manager) - 查看检查点\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f832d",
   "metadata": {},
   "source": [
    "# Training from scratch\n",
    "First trained with SNR 200 data, then continue training with SNR 50, 100 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4834ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载 135649 个文件\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# init the dataset\n",
    "data_folder = \"/home/ud202180035/DNN_for_GW_Parameter_Space_Estimation/waveforms2\"  # 你的数据目录，可按需修改\n",
    "target_snr = [\"200.00\"]  # 可指定加载哪些 SNR 数据\n",
    "\n",
    "dataset = GWDataset(data_folder, target_snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b79842",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = split_dataset(dataset)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True,  num_workers=24, pin_memory=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=32, shuffle=False, num_workers=24, pin_memory=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=32, shuffle=False, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3866f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WaveUNetWithTransformer(\n",
       "  (encoder1): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder2): Sequential(\n",
       "    (0): Conv1d(16, 32, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder3): Sequential(\n",
       "    (0): Conv1d(32, 64, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder4): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (transformer_input_proj): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_output_proj): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "  (decoder1): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(128, 64, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder2): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(64, 32, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder3): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(32, 16, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder4): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(16, 1, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 初始化model\n",
    "denoising_model = WaveUNetWithTransformer()\n",
    "denoising_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229cb714",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(denoising_model.parameters(), lr=1e-5, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa55101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "best_loss = train_model_with_checkpoints(\n",
    "    model=denoising_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=200,\n",
    "    checkpoint_manager=checkpoint_manager,\n",
    "    resume_from_checkpoint=None,\n",
    "    save_every_n_epochs=10,\n",
    "    training_config={\n",
    "        \"snr\": [\"200.00\"],\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"weight_decay\": 1e-6,\n",
    "        \"save_every_n_epochs\": 10,\n",
    "        \"number of epochs\": 200\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"✅ 训练完成! 最佳验证损失: {best_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700a750",
   "metadata": {},
   "source": [
    "# Continue training from best_model or resume from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc20450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载 135649 个文件\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# init the dataset\n",
    "data_folder = \"/home/ud202180035/DNN_for_GW_Parameter_Space_Estimation/waveforms2\"  # 你的数据目录，可按需修改\n",
    "target_snr = [\"50\",\"100\"]  # 可指定加载哪些 SNR 数据\n",
    "\n",
    "dataset = GWDataset(data_folder, target_snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62059db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = split_dataset(dataset)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True,  num_workers=24, pin_memory=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=32, shuffle=False, num_workers=24, pin_memory=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=32, shuffle=False, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5482a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8720530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WaveUNetWithTransformer(\n",
       "  (encoder1): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder2): Sequential(\n",
       "    (0): Conv1d(16, 32, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder3): Sequential(\n",
       "    (0): Conv1d(32, 64, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder4): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (transformer_input_proj): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_output_proj): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "  (decoder1): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(128, 64, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder2): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(64, 32, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder3): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(32, 16, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder4): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='linear')\n",
       "    (1): Conv1d(16, 1, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 初始化model\n",
    "denoising_model = WaveUNetWithTransformer()\n",
    "denoising_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9243af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "checkpoint_manager.load_best_model(denoising_model,\"best_denoising_model.pt\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948abef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建新的优化器（可以使用较小的学习率进行微调）\n",
    "optimizer = torch.optim.Adam(denoising_model.parameters(), lr=1e-6, weight_decay=1e-6)  # 更小的学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43cb855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ud202180035/miniconda3/envs/dnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ 加载检查点失败: 检查点文件不存在: model_checkpoints/checkpoint_epoch_40_20250710_164543.pt\n",
      "🆕 从头开始训练\n",
      "🚀 开始训练:\n",
      "   - 模型: WaveUNetWithTransformer\n",
      "   - 总参数量: 2,497,729\n",
      "   - 可训练参数: 2,497,729\n",
      "   - 设备: cuda\n",
      "   - 训练轮次: 0 -> 60\n",
      "   - 学习率: 1e-06\n",
      "  1/60: Train Loss = 26.278693, Val Loss = 2.468107, Output Std = 0.2427, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.468107)\n",
      "  🏆 Best model saved!\n",
      "  2/60: Train Loss = 2.291680, Val Loss = 2.198200, Output Std = 0.2472, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.198200)\n",
      "  🏆 Best model saved!\n",
      "  3/60: Train Loss = 2.197308, Val Loss = 2.170144, Output Std = 0.2492, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.170144)\n",
      "  🏆 Best model saved!\n",
      "  4/60: Train Loss = 2.179067, Val Loss = 2.157876, Output Std = 0.2483, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.157876)\n",
      "  🏆 Best model saved!\n",
      "  5/60: Train Loss = 2.168617, Val Loss = 2.156744, Output Std = 0.2521, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.156744)\n",
      "  🏆 Best model saved!\n",
      "  6/60: Train Loss = 2.162716, Val Loss = 2.144276, Output Std = 0.2510, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.144276)\n",
      "  🏆 Best model saved!\n",
      "  7/60: Train Loss = 2.154733, Val Loss = 2.136824, Output Std = 0.2504, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.136824)\n",
      "  🏆 Best model saved!\n",
      "  8/60: Train Loss = 2.148793, Val Loss = 2.131226, Output Std = 0.2478, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.131226)\n",
      "  🏆 Best model saved!\n",
      "  9/60: Train Loss = 2.141751, Val Loss = 2.125323, Output Std = 0.2497, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.125323)\n",
      "  🏆 Best model saved!\n",
      " 10/60: Train Loss = 2.139210, Val Loss = 2.122159, Output Std = 0.2510, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.122159)\n",
      "  🏆 Best model saved!\n",
      "✅ 检查点已保存: model_checkpoints/checkpoint_epoch_10_20250723_173335.pt\n",
      " 11/60: Train Loss = 2.131683, Val Loss = 2.116201, Output Std = 0.2507, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.116201)\n",
      "  🏆 Best model saved!\n",
      " 12/60: Train Loss = 2.125454, Val Loss = 2.109856, Output Std = 0.2500, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.109856)\n",
      "  🏆 Best model saved!\n",
      " 13/60: Train Loss = 2.120737, Val Loss = 2.103640, Output Std = 0.2472, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.103640)\n",
      "  🏆 Best model saved!\n",
      " 14/60: Train Loss = 2.114977, Val Loss = 2.098335, Output Std = 0.2473, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.098335)\n",
      "  🏆 Best model saved!\n",
      " 15/60: Train Loss = 2.108821, Val Loss = 2.093383, Output Std = 0.2491, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.093383)\n",
      "  🏆 Best model saved!\n",
      " 16/60: Train Loss = 2.104333, Val Loss = 2.090895, Output Std = 0.2496, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.090895)\n",
      "  🏆 Best model saved!\n",
      " 17/60: Train Loss = 2.099904, Val Loss = 2.085953, Output Std = 0.2471, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.085953)\n",
      "  🏆 Best model saved!\n",
      " 18/60: Train Loss = 2.097093, Val Loss = 2.084296, Output Std = 0.2491, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.084296)\n",
      "  🏆 Best model saved!\n",
      " 19/60: Train Loss = 2.094248, Val Loss = 2.080151, Output Std = 0.2479, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.080151)\n",
      "  🏆 Best model saved!\n",
      " 20/60: Train Loss = 2.091811, Val Loss = 2.078814, Output Std = 0.2481, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.078814)\n",
      "  🏆 Best model saved!\n",
      "✅ 检查点已保存: model_checkpoints/checkpoint_epoch_20_20250723_233219.pt\n",
      " 21/60: Train Loss = 2.090958, Val Loss = 2.076461, Output Std = 0.2471, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.076461)\n",
      "  🏆 Best model saved!\n",
      " 22/60: Train Loss = 2.088698, Val Loss = 2.076264, Output Std = 0.2463, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.076264)\n",
      "  🏆 Best model saved!\n",
      " 23/60: Train Loss = 2.086484, Val Loss = 2.075483, Output Std = 0.2490, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.075483)\n",
      "  🏆 Best model saved!\n",
      " 24/60: Train Loss = 2.084955, Val Loss = 2.073728, Output Std = 0.2464, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.073728)\n",
      "  🏆 Best model saved!\n",
      " 25/60: Train Loss = 2.084319, Val Loss = 2.071787, Output Std = 0.2477, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.071787)\n",
      "  🏆 Best model saved!\n",
      " 26/60: Train Loss = 2.082461, Val Loss = 2.077116, Output Std = 0.2505, LR = 1.00e-06\n",
      " 27/60: Train Loss = 2.082282, Val Loss = 2.070191, Output Std = 0.2471, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.070191)\n",
      "  🏆 Best model saved!\n",
      " 28/60: Train Loss = 2.081075, Val Loss = 2.069242, Output Std = 0.2482, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.069242)\n",
      "  🏆 Best model saved!\n",
      " 29/60: Train Loss = 2.081207, Val Loss = 2.068537, Output Std = 0.2484, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.068537)\n",
      "  🏆 Best model saved!\n",
      " 30/60: Train Loss = 2.079419, Val Loss = 2.071894, Output Std = 0.2501, LR = 1.00e-06\n",
      "✅ 检查点已保存: model_checkpoints/checkpoint_epoch_30_20250724_052846.pt\n",
      " 31/60: Train Loss = 2.077845, Val Loss = 2.068021, Output Std = 0.2487, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.068021)\n",
      "  🏆 Best model saved!\n",
      " 32/60: Train Loss = 2.078468, Val Loss = 2.067798, Output Std = 0.2492, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.067798)\n",
      "  🏆 Best model saved!\n",
      " 33/60: Train Loss = 2.077842, Val Loss = 2.067637, Output Std = 0.2487, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.067637)\n",
      "  🏆 Best model saved!\n",
      " 34/60: Train Loss = 2.077693, Val Loss = 2.067030, Output Std = 0.2493, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.067030)\n",
      "  🏆 Best model saved!\n",
      " 35/60: Train Loss = 2.076178, Val Loss = 2.065321, Output Std = 0.2476, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.065321)\n",
      "  🏆 Best model saved!\n",
      " 36/60: Train Loss = 2.075812, Val Loss = 2.064727, Output Std = 0.2475, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.064727)\n",
      "  🏆 Best model saved!\n",
      " 37/60: Train Loss = 2.075676, Val Loss = 2.064232, Output Std = 0.2478, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.064232)\n",
      "  🏆 Best model saved!\n",
      " 38/60: Train Loss = 2.075652, Val Loss = 2.064529, Output Std = 0.2490, LR = 1.00e-06\n",
      " 39/60: Train Loss = 2.074099, Val Loss = 2.063961, Output Std = 0.2474, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.063961)\n",
      "  🏆 Best model saved!\n",
      " 40/60: Train Loss = 2.073581, Val Loss = 2.063130, Output Std = 0.2476, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.063130)\n",
      "  🏆 Best model saved!\n",
      "✅ 检查点已保存: model_checkpoints/checkpoint_epoch_40_20250724_112444.pt\n",
      " 41/60: Train Loss = 2.073601, Val Loss = 2.062955, Output Std = 0.2473, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.062955)\n",
      "  🏆 Best model saved!\n",
      " 42/60: Train Loss = 2.073411, Val Loss = 2.062614, Output Std = 0.2486, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.062614)\n",
      "  🏆 Best model saved!\n",
      " 43/60: Train Loss = 2.073151, Val Loss = 2.062433, Output Std = 0.2486, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.062433)\n",
      "  🏆 Best model saved!\n",
      " 44/60: Train Loss = 2.072010, Val Loss = 2.061690, Output Std = 0.2482, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.061690)\n",
      "  🏆 Best model saved!\n",
      " 45/60: Train Loss = 2.072743, Val Loss = 2.061618, Output Std = 0.2486, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.061618)\n",
      "  🏆 Best model saved!\n",
      " 46/60: Train Loss = 2.072353, Val Loss = 2.061198, Output Std = 0.2475, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.061198)\n",
      "  🏆 Best model saved!\n",
      " 47/60: Train Loss = 2.071891, Val Loss = 2.061449, Output Std = 0.2489, LR = 1.00e-06\n",
      " 48/60: Train Loss = 2.071164, Val Loss = 2.061219, Output Std = 0.2478, LR = 1.00e-06\n",
      " 49/60: Train Loss = 2.071487, Val Loss = 2.061932, Output Std = 0.2494, LR = 1.00e-06\n",
      " 50/60: Train Loss = 2.071574, Val Loss = 2.061373, Output Std = 0.2493, LR = 1.00e-06\n",
      "✅ 检查点已保存: model_checkpoints/checkpoint_epoch_50_20250724_172205.pt\n",
      "🗑️ 已删除旧检查点: checkpoint_epoch_10_20250723_173335.pt\n",
      "🗑️ 已删除旧检查点: checkpoint_epoch_20_20250723_233219.pt\n",
      "🗑️ 已删除旧检查点: checkpoint_epoch_30_20250724_052846.pt\n",
      "🗑️ 已删除旧检查点: checkpoint_epoch_40_20250724_112444.pt\n",
      "🗑️ 已删除旧检查点: checkpoint_epoch_50_20250724_172205.pt\n",
      "🗑️ 已删除旧检查点: checkpoint_epoch_60_20250711_043605.pt\n",
      "🗑️ 已删除旧检查点: checkpoint_epoch_70_20250711_103159.pt\n",
      "✅ 清理完成，保留了最近的 5 个检查点\n",
      " 51/60: Train Loss = 2.070379, Val Loss = 2.060905, Output Std = 0.2491, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.060905)\n",
      "  🏆 Best model saved!\n",
      " 52/60: Train Loss = 2.070767, Val Loss = 2.059791, Output Std = 0.2481, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.059791)\n",
      "  🏆 Best model saved!\n",
      " 53/60: Train Loss = 2.070148, Val Loss = 2.060374, Output Std = 0.2472, LR = 1.00e-06\n",
      " 54/60: Train Loss = 2.070471, Val Loss = 2.059828, Output Std = 0.2488, LR = 1.00e-06\n",
      " 55/60: Train Loss = 2.069950, Val Loss = 2.064241, Output Std = 0.2505, LR = 1.00e-06\n",
      " 56/60: Train Loss = 2.070581, Val Loss = 2.059057, Output Std = 0.2479, LR = 1.00e-06\n",
      "🏆 最佳模型已保存: model_checkpoints/best_denoising_model.pt (验证损失: 2.059057)\n",
      "  🏆 Best model saved!\n",
      " 57/60: Train Loss = 2.070071, Val Loss = 2.059583, Output Std = 0.2489, LR = 1.00e-06\n",
      " 58/60: Train Loss = 2.069849, Val Loss = 2.059489, Output Std = 0.2491, LR = 1.00e-06\n",
      " 59/60: Train Loss = 2.068840, Val Loss = 2.059451, Output Std = 0.2471, LR = 1.00e-06\n",
      " 60/60: Train Loss = 2.069535, Val Loss = 2.061387, Output Std = 0.2499, LR = 1.00e-06\n",
      "✅ 检查点已保存: model_checkpoints/checkpoint_epoch_60_20250724_232348.pt\n",
      "✅ 检查点已保存: model_checkpoints/final_checkpoint_epoch_60.pt\n",
      "🎯 训练完成! 最终检查点: model_checkpoints/final_checkpoint_epoch_60.pt\n",
      "✅ 训练完成! 最佳验证损失: 2.059057\n"
     ]
    }
   ],
   "source": [
    "# 继续训练\n",
    "best_loss = train_model_with_checkpoints(\n",
    "            model=denoising_model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            num_epochs=60,  # 微调轮次\n",
    "            checkpoint_manager=checkpoint_manager,\n",
    "            save_every_n_epochs=10,\n",
    "            training_config={\n",
    "                'fine_tuning': True,\n",
    "                'original_data': \"waveforms2\",\n",
    "                'new_data': \"waveforms2\",\n",
    "                'new_snr': [\"50\",\"100\"],\n",
    "                'learning_rate': optimizer.param_groups[0]['lr']\n",
    "            }\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4gw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
