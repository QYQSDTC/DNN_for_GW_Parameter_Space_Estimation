{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as pl\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR 50.00 - white_Data shape: (18000, 6184)\n",
      "SNR 50.00 - white_Signal shape: (18000, 6184)\n",
      "SNR 50.00 - Attributes: {'dl_true': 1.0910208776455039e+19, 'eta_true': 0.2410471954822854, 'iota_true': 1.2747004358482887, 'mc_true': 201.3973973282873, 'phic_true': 5.603175015853413, 'phis_true': 2.661901610522322, 'psi_true': 2.749441536415439, 'snr': 50, 'tc_true': 81014.19034346812, 'thetas_true': 1.48090896530732}\n",
      "SNR 100.00 - white_Data shape: (18000, 6184)\n",
      "SNR 100.00 - white_Signal shape: (18000, 6184)\n",
      "SNR 100.00 - Attributes: {'dl_true': 1.0910208776455039e+19, 'eta_true': 0.2345313550836741, 'iota_true': 2.0045559545930582, 'mc_true': 117.15661887775255, 'phic_true': 3.562486464513828, 'phis_true': 0.7690836855688769, 'psi_true': 0.17080533179531923, 'snr': 100, 'tc_true': 72421.7463165898, 'thetas_true': 0.9723749157083255}\n",
      "SNR 200.00 - white_Data shape: (17999, 6184)\n",
      "SNR 200.00 - white_Signal shape: (17999, 6184)\n",
      "SNR 200.00 - Attributes: {'dl_true': 1.0910208776455039e+19, 'eta_true': 0.2345313550836741, 'iota_true': 2.0045559545930582, 'mc_true': 117.15661887775255, 'phic_true': 3.562486464513828, 'phis_true': 0.7690836855688769, 'psi_true': 0.17080533179531923, 'snr': 200, 'tc_true': 72421.7463165898, 'thetas_true': 0.9723749157083255}\n"
     ]
    }
   ],
   "source": [
    "# 设置文件夹路径和样本名称\n",
    "folder_path = 'D:/data/waveforms2/'\n",
    "\n",
    "# 创建字典来存储不同 SNR 值下的样本数据和属性信息\n",
    "all_samples_by_snr = {\n",
    "    '50.00': {'white_Data': [], 'white_Signal': [], 'attributes': []},\n",
    "    '100.00': {'white_Data': [], 'white_Signal': [], 'attributes': []},\n",
    "    '200.00': {'white_Data': [], 'white_Signal': [], 'attributes': []}\n",
    "}\n",
    "\n",
    "# 正则表达式匹配 SNR 值\n",
    "snr_pattern = re.compile(r'_SNR(\\d+\\.\\d+)\\.h5')\n",
    "\n",
    "# 遍历文件夹中的每个文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    # 确保只处理 .h5 文件\n",
    "    if filename.endswith('.h5'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # 提取 SNR 值\n",
    "        match = snr_pattern.search(filename)\n",
    "        if match:\n",
    "            snr_value = match.group(1)  # 获取 SNR 值（如 '50.00'）\n",
    "            \n",
    "            # 检查是否在预期的 SNR 值中\n",
    "            if snr_value in all_samples_by_snr:\n",
    "                try:\n",
    "                    with h5py.File(file_path, 'r') as file:\n",
    "                        # 检查文件中是否包含 'data' 数据集\n",
    "                        if 'Data' in file:\n",
    "                            Data = file['Data']\n",
    "                            \n",
    "                            # 读取样本数据\n",
    "                            if isinstance(Data, h5py.Dataset):\n",
    "                                sample_1 = Data[0].flatten()  # 将样本扁平化为一维数组\n",
    "                                sample_2 = Data[1].flatten()  # 将样本扁平化为一维数组\n",
    "                            elif isinstance(Data, h5py.Group):\n",
    "                                # 如果 'data' 是组\n",
    "                                sample_1 = Data['white_Data'][:]\n",
    "                                sample_2 = Data['white_signal'][:]\n",
    "                            else:\n",
    "                                print(f\"Unexpected data type in {filename}\")\n",
    "                                continue\n",
    "                            \n",
    "                            # 读取属性信息\n",
    "                            attributes = {attr: Data.attrs[attr] for attr in Data.attrs}\n",
    "                            \n",
    "                            # 将样本数据和属性信息添加到对应的 SNR 类别中\n",
    "                            all_samples_by_snr[snr_value]['white_Data'].append(sample_1)\n",
    "                            all_samples_by_snr[snr_value]['white_Signal'].append(sample_2)\n",
    "                            all_samples_by_snr[snr_value]['attributes'].append(attributes)\n",
    "                        else:\n",
    "                            print(f\"'Data' dataset not found in {filename}\")\n",
    "                \n",
    "                # 捕获并报告文件无法读取的错误\n",
    "                except (OSError, KeyError) as e:\n",
    "                    print(f\"Error reading file {filename}: {e}\")\n",
    "\n",
    "# 将每个 SNR 类别的样本列表拼接成完整的数据集\n",
    "combined_datasets = {}\n",
    "for snr_value, samples in all_samples_by_snr.items():\n",
    "    combined_datasets[snr_value] = {\n",
    "        'white_Data': np.array(samples['white_Data']),\n",
    "        'white_Signal': np.array(samples['white_Signal']),\n",
    "        'attributes': samples['attributes']  # 保留属性信息\n",
    "    }\n",
    "    print(f\"SNR {snr_value} - white_Data shape:\", combined_datasets[snr_value]['white_Data'].shape)\n",
    "    print(f\"SNR {snr_value} - white_Signal shape:\", combined_datasets[snr_value]['white_Signal'].shape)\n",
    "    print(f\"SNR {snr_value} - Attributes:\", combined_datasets[snr_value]['attributes'][0])  # 显示第一个文件的属性信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_SNR50 shape: (18000, 1)\n",
      "target_SNR50 values: [[201.39739733]\n",
      " [117.15661888]\n",
      " [117.15661888]\n",
      " ...\n",
      " [201.39739733]\n",
      " [201.39739733]\n",
      " [201.39739733]]\n",
      "target_SNR100 shape: (18000, 1)\n",
      "target_SNR100 values: [[117.15661888]\n",
      " [117.15661888]\n",
      " [117.15661888]\n",
      " ...\n",
      " [201.39739733]\n",
      " [201.39739733]\n",
      " [201.39739733]]\n",
      "target_SNR200 shape: (17999, 1)\n",
      "target_SNR200 values: [[117.15661888]\n",
      " [117.15661888]\n",
      " [117.15661888]\n",
      " ...\n",
      " [201.39739733]\n",
      " [201.39739733]\n",
      " [201.39739733]]\n"
     ]
    }
   ],
   "source": [
    "if '50.00' in combined_datasets:\n",
    "    target_values = []\n",
    "\n",
    "    # 遍历每个样本的属性，提取 mc 的值\n",
    "    for attributes in combined_datasets['50.00']['attributes']:\n",
    "        target_value = attributes.get('mc_true')  # 获取 mc 的值\n",
    "#         target_value = attributes.get('tc_true')  # 获取 tc 的值\n",
    "        if target_value is not None:\n",
    "            target_values.append(target_value)\n",
    "\n",
    "    # 创建 shape 为 (样本数, 1) 的数组\n",
    "    target_SNR50 = np.array(target_values).reshape(-1, 1)\n",
    "\n",
    "    print(\"target_SNR50 shape:\", target_SNR50.shape)\n",
    "    print(\"target_SNR50 values:\", target_SNR50)\n",
    "\n",
    "if '100.00' in combined_datasets:\n",
    "    target_values = []\n",
    "\n",
    "    # 遍历每个样本的属性，提取 mc 的值\n",
    "    for attributes in combined_datasets['100.00']['attributes']:\n",
    "        target_value = attributes.get('mc_true')  # 获取 mc 的值\n",
    "#         target_value = attributes.get('tc_true')  # 获取 tc 的值\n",
    "        if target_value is not None:\n",
    "            target_values.append(target_value)\n",
    "\n",
    "    # 创建 shape 为 (样本数, 1) 的数组\n",
    "    target_SNR100 = np.array(target_values).reshape(-1, 1)\n",
    "\n",
    "    print(\"target_SNR100 shape:\", target_SNR100.shape)\n",
    "    print(\"target_SNR100 values:\", target_SNR100)\n",
    "    \n",
    "if '200.00' in combined_datasets:\n",
    "    target_values = []\n",
    "\n",
    "    # 遍历每个样本的属性，提取 mc 的值\n",
    "    for attributes in combined_datasets['200.00']['attributes']:\n",
    "        target_value = attributes.get('mc_true')  # 获取 mc 的值\n",
    "#         target_value = attributes.get('tc_true')  # 获取 tc 的值\n",
    "        if target_value is not None:\n",
    "            target_values.append(target_value)\n",
    "\n",
    "    # 创建 shape 为 (样本数, 1) 的数组\n",
    "    target_SNR200 = np.array(target_values).reshape(-1, 1)\n",
    "\n",
    "    print(\"target_SNR200 shape:\", target_SNR200.shape)\n",
    "    print(\"target_SNR200 values:\", target_SNR200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_SNR50 shape: (18000, 6184)\n",
      "data_SNR50 values: [[ 0.03137989  0.25731843 -0.27357243 ...  0.50245534 -0.05588465\n",
      "  -0.02275486]\n",
      " [-0.36055721  0.02079585 -0.11857392 ...  0.43280806 -0.05776062\n",
      "  -0.22265224]\n",
      " [-0.09969713 -0.13879934 -0.04909878 ...  0.16727563  0.5150171\n",
      "   0.26231936]\n",
      " ...\n",
      " [-0.35501129 -0.4650888  -0.30697662 ... -0.15451082 -0.25009854\n",
      "   0.17222949]\n",
      " [ 0.10760243 -0.03824991  0.0096525  ... -0.51311919 -0.32216384\n",
      "  -0.18480158]\n",
      " [ 0.28727159 -0.29340144  0.04059797 ... -0.15664948 -0.5164065\n",
      "   0.03016353]]\n"
     ]
    }
   ],
   "source": [
    "data_SNR50 = combined_datasets['50.00']['white_Data']\n",
    "signal_SNR50 = combined_datasets['50.00']['white_Signal']\n",
    "\n",
    "data_SNR100 = combined_datasets['100.00']['white_Data']\n",
    "signal_SNR100 = combined_datasets['100.00']['white_Signal']\n",
    "\n",
    "data_SNR200 = combined_datasets['200.00']['white_Data']\n",
    "signal_SNR200 = combined_datasets['200.00']['white_Signal']\n",
    "\n",
    "print(\"data_SNR50 shape:\", data_SNR50.shape)\n",
    "print(\"data_SNR50 values:\", data_SNR50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: (14399, 6184) (14399, 1)\n",
      "验证集: (1800, 6184) (1800, 1)\n",
      "测试集: (1800, 6184) (1800, 1)\n"
     ]
    }
   ],
   "source": [
    "data = signal_SNR200  #  选择数据\n",
    "label = target_SNR200  #  选择标签\n",
    "\n",
    "#  划分数据集\n",
    "x_train, x_valtest, y_train, y_valtest = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_valtest, y_valtest, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"训练集:\", x_train.shape, y_train.shape)\n",
    "print(\"验证集:\", x_val.shape, y_val.shape)\n",
    "print(\"测试集:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19784915,  0.19734082,  0.19678974, ...,  1.54332901,\n",
       "         1.5391046 ,  1.5467802 ],\n",
       "       [ 0.45710036,  0.45412562,  0.45111574, ...,  1.53885862,\n",
       "         1.54794727,  1.5196064 ],\n",
       "       [ 0.35817274,  0.34568083,  0.33321574, ...,  0.47958742,\n",
       "         0.49146276,  0.45847318],\n",
       "       ...,\n",
       "       [ 0.20237902,  0.20298519,  0.20355837, ...,  0.96407423,\n",
       "         0.94234758,  1.00204403],\n",
       "       [ 0.45846104,  0.45885631,  0.45921656, ...,  0.43965176,\n",
       "         0.43246186,  0.45640482],\n",
       "       [-0.65425841, -0.65784239, -0.66136536, ..., -0.40504931,\n",
       "        -0.37743084, -0.44672226]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  标准化\n",
    "std = StandardScaler().fit(x_train)\n",
    "x_train = std.transform(x_train)\n",
    "x_val = std.transform(x_val)\n",
    "x_test = std.transform(x_test)\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.59461128, -0.35259493, -1.33779135, ...,  2.54983522,\n",
       "       -0.75734024, -0.86193809])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_std = StandardScaler().fit(y_train)\n",
    "y_train = label_std.transform(y_train)\n",
    "y_val = label_std.transform(y_val)\n",
    "y_test = label_std.transform(y_test)\n",
    "\n",
    "y_train = y_train.reshape(-1)\n",
    "y_val = y_val.reshape(-1)\n",
    "y_test = y_test.reshape(-1)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([14399, 1, 6184]),\n",
       " torch.Size([1800, 1, 6184]),\n",
       " torch.Size([1800, 1, 6184]),\n",
       " torch.Size([14399]),\n",
       " torch.Size([1800]),\n",
       " torch.Size([1800]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = torch.unsqueeze(torch.from_numpy(x_train), dim=1).to(torch.float32)\n",
    "x_val = torch.unsqueeze(torch.from_numpy(x_val), dim=1).to(torch.float32)\n",
    "x_test = torch.unsqueeze(torch.from_numpy(x_test), dim=1).to(torch.float32)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).to(torch.float32)\n",
    "y_val = torch.from_numpy(y_val).to(torch.float32)\n",
    "y_test = torch.from_numpy(y_test).to(torch.float32)\n",
    "\n",
    "x_train.shape, x_val.shape, x_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=7, stride=stride, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self.make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, 1)  # 输出一个值\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNetModel():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=2, base_channels=64):\n",
    "        super(WaveUNet, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels, base_channels)\n",
    "        self.encoder2 = self.conv_block(base_channels, base_channels * 2)\n",
    "        self.encoder3 = self.conv_block(base_channels * 2, base_channels * 4)\n",
    "        self.encoder4 = self.conv_block(base_channels * 4, base_channels * 8)\n",
    "        self.encoder5 = self.conv_block(base_channels * 8, base_channels * 16)\n",
    "        self.encoder6 = self.conv_block(base_channels * 16, base_channels * 32)\n",
    "        self.encoder7 = self.conv_block(base_channels * 32, base_channels * 64)\n",
    "        self.encoder8 = self.conv_block(base_channels * 64, base_channels * 128)\n",
    "\n",
    "        self.decoder8 = self.conv_block(base_channels * 128, base_channels * 64)\n",
    "        self.decoder7 = self.conv_block(base_channels * 64, base_channels * 32)\n",
    "        self.decoder6 = self.conv_block(base_channels * 32, base_channels * 16)\n",
    "        self.decoder5 = self.conv_block(base_channels * 16, base_channels * 8)\n",
    "        self.decoder4 = self.conv_block(base_channels * 8, base_channels * 4)\n",
    "        self.decoder3 = self.conv_block(base_channels * 4, base_channels * 2)\n",
    "        self.decoder2 = self.conv_block(base_channels * 2, base_channels)\n",
    "        self.decoder1 = self.conv_block(base_channels, out_channels, final_layer=True)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.upconv8 = nn.ConvTranspose1d(base_channels * 128, base_channels * 64, kernel_size=2, stride=2)\n",
    "        self.upconv7 = nn.ConvTranspose1d(base_channels * 64, base_channels * 32, kernel_size=2, stride=2)\n",
    "        self.upconv6 = nn.ConvTranspose1d(base_channels * 32, base_channels * 16, kernel_size=2, stride=2)\n",
    "        self.upconv5 = nn.ConvTranspose1d(base_channels * 16, base_channels * 8, kernel_size=2, stride=2)\n",
    "        self.upconv4 = nn.ConvTranspose1d(base_channels * 8, base_channels * 4, kernel_size=2, stride=2)\n",
    "        self.upconv3 = nn.ConvTranspose1d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2)\n",
    "        self.upconv2 = nn.ConvTranspose1d(base_channels * 2, base_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels, final_layer=False):\n",
    "        layers = [nn.Conv1d(in_channels, out_channels, kernel_size=15, padding=7),  # 使用较大的卷积核15\n",
    "                  nn.ReLU(inplace=True)]\n",
    "        \n",
    "        if not final_layer:\n",
    "            layers.append(nn.Conv1d(out_channels, out_channels, kernel_size=15, padding=7))  # 使用较大的卷积核15\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        enc4 = self.encoder4(self.pool(enc3))\n",
    "        enc5 = self.encoder5(self.pool(enc4))\n",
    "        enc6 = self.encoder6(self.pool(enc5))\n",
    "        enc7 = self.encoder7(self.pool(enc6))\n",
    "        enc8 = self.encoder8(self.pool(enc7))\n",
    "\n",
    "        dec8 = self.upconv8(enc8)\n",
    "        dec8 = torch.cat([dec8, enc7], dim=1)\n",
    "        dec8 = self.decoder8(dec8)\n",
    "\n",
    "        dec7 = self.upconv7(dec8)\n",
    "        if dec7.size(2) != enc6.size(2):\n",
    "            enc6 = F.pad(enc6, (0, dec7.size(2) - enc6.size(2)))\n",
    "        dec7 = torch.cat([dec7, enc6], dim=1)\n",
    "        dec7 = self.decoder7(dec7)\n",
    "\n",
    "        dec6 = self.upconv6(dec7)\n",
    "        if dec6.size(2) != enc5.size(2):\n",
    "            enc5 = F.pad(enc5, (0, dec6.size(2) - enc5.size(2)))\n",
    "        dec6 = torch.cat([dec6, enc5], dim=1)\n",
    "        dec6 = self.decoder6(dec6)\n",
    "\n",
    "        dec5 = self.upconv5(dec6)\n",
    "        if dec5.size(2) != enc4.size(2):\n",
    "            enc4 = F.pad(enc4, (0, dec5.size(2) - enc4.size(2)))\n",
    "        dec5 = torch.cat([dec5, enc4], dim=1)\n",
    "        dec5 = self.decoder5(dec5)\n",
    "\n",
    "        dec4 = self.upconv4(dec5)\n",
    "        if dec4.size(2) != enc3.size(2):\n",
    "            enc3 = F.pad(enc3, (0, dec4.size(2) - enc3.size(2)))\n",
    "        dec4 = torch.cat([dec4, enc3], dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        if dec3.size(2) != enc2.size(2):\n",
    "            enc2 = F.pad(enc2, (0, dec3.size(2) - enc2.size(2)))\n",
    "        dec3 = torch.cat([dec3, enc2], dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        if dec2.size(2) != enc1.size(2):\n",
    "            enc1 = F.pad(enc1, (0, dec2.size(2) - enc1.size(2)))\n",
    "        dec2 = torch.cat([dec2, enc1], dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        dec1 = self.decoder1(dec2)\n",
    "        \n",
    "        return dec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, wave_u_net, pred_model):\n",
    "        super(Model, self).__init__()\n",
    "        self.wave_u_net = wave_u_net\n",
    "        self.pred_model = pred_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        denoised_output = self.wave_u_net(x)\n",
    "        denoised_signal = denoised_output[:, 0, :]\n",
    "\n",
    "        predictions = self.pred_model(denoised_signal.unsqueeze(1))  # 添加通道维度\n",
    "        return predictions\n",
    "\n",
    "# 初始化模型\n",
    "wave_u_net = WaveUNet(in_channels=1, out_channels=2, base_channels=64)\n",
    "pred_model = ResNetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(wave_u_net, pred_model)\n",
    "# model = ResNetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 64, 6184]           1,024\n",
      "              ReLU-2             [-1, 64, 6184]               0\n",
      "            Conv1d-3             [-1, 64, 6184]          61,504\n",
      "              ReLU-4             [-1, 64, 6184]               0\n",
      "         MaxPool1d-5             [-1, 64, 3092]               0\n",
      "            Conv1d-6            [-1, 128, 3092]         123,008\n",
      "              ReLU-7            [-1, 128, 3092]               0\n",
      "            Conv1d-8            [-1, 128, 3092]         245,888\n",
      "              ReLU-9            [-1, 128, 3092]               0\n",
      "        MaxPool1d-10            [-1, 128, 1546]               0\n",
      "           Conv1d-11            [-1, 256, 1546]         491,776\n",
      "             ReLU-12            [-1, 256, 1546]               0\n",
      "           Conv1d-13            [-1, 256, 1546]         983,296\n",
      "             ReLU-14            [-1, 256, 1546]               0\n",
      "        MaxPool1d-15             [-1, 256, 773]               0\n",
      "           Conv1d-16             [-1, 512, 773]       1,966,592\n",
      "             ReLU-17             [-1, 512, 773]               0\n",
      "           Conv1d-18             [-1, 512, 773]       3,932,672\n",
      "             ReLU-19             [-1, 512, 773]               0\n",
      "        MaxPool1d-20             [-1, 512, 386]               0\n",
      "           Conv1d-21            [-1, 1024, 386]       7,865,344\n",
      "             ReLU-22            [-1, 1024, 386]               0\n",
      "           Conv1d-23            [-1, 1024, 386]      15,729,664\n",
      "             ReLU-24            [-1, 1024, 386]               0\n",
      "        MaxPool1d-25            [-1, 1024, 193]               0\n",
      "           Conv1d-26            [-1, 2048, 193]      31,459,328\n",
      "             ReLU-27            [-1, 2048, 193]               0\n",
      "           Conv1d-28            [-1, 2048, 193]      62,916,608\n",
      "             ReLU-29            [-1, 2048, 193]               0\n",
      "        MaxPool1d-30             [-1, 2048, 96]               0\n",
      "           Conv1d-31             [-1, 4096, 96]     125,833,216\n",
      "             ReLU-32             [-1, 4096, 96]               0\n",
      "           Conv1d-33             [-1, 4096, 96]     251,662,336\n",
      "             ReLU-34             [-1, 4096, 96]               0\n",
      "        MaxPool1d-35             [-1, 4096, 48]               0\n",
      "           Conv1d-36             [-1, 8192, 48]     503,324,672\n",
      "             ReLU-37             [-1, 8192, 48]               0\n",
      "           Conv1d-38             [-1, 8192, 48]   1,006,641,152\n",
      "             ReLU-39             [-1, 8192, 48]               0\n",
      "  ConvTranspose1d-40             [-1, 4096, 96]      67,112,960\n",
      "           Conv1d-41             [-1, 4096, 96]     503,320,576\n",
      "             ReLU-42             [-1, 4096, 96]               0\n",
      "           Conv1d-43             [-1, 4096, 96]     251,662,336\n",
      "             ReLU-44             [-1, 4096, 96]               0\n",
      "  ConvTranspose1d-45            [-1, 2048, 192]      16,779,264\n",
      "           Conv1d-46            [-1, 2048, 192]     125,831,168\n",
      "             ReLU-47            [-1, 2048, 192]               0\n",
      "           Conv1d-48            [-1, 2048, 192]      62,916,608\n",
      "             ReLU-49            [-1, 2048, 192]               0\n",
      "  ConvTranspose1d-50            [-1, 1024, 384]       4,195,328\n",
      "           Conv1d-51            [-1, 1024, 384]      31,458,304\n",
      "             ReLU-52            [-1, 1024, 384]               0\n",
      "           Conv1d-53            [-1, 1024, 384]      15,729,664\n",
      "             ReLU-54            [-1, 1024, 384]               0\n",
      "  ConvTranspose1d-55             [-1, 512, 768]       1,049,088\n",
      "           Conv1d-56             [-1, 512, 768]       7,864,832\n",
      "             ReLU-57             [-1, 512, 768]               0\n",
      "           Conv1d-58             [-1, 512, 768]       3,932,672\n",
      "             ReLU-59             [-1, 512, 768]               0\n",
      "  ConvTranspose1d-60            [-1, 256, 1536]         262,400\n",
      "           Conv1d-61            [-1, 256, 1536]       1,966,336\n",
      "             ReLU-62            [-1, 256, 1536]               0\n",
      "           Conv1d-63            [-1, 256, 1536]         983,296\n",
      "             ReLU-64            [-1, 256, 1536]               0\n",
      "  ConvTranspose1d-65            [-1, 128, 3072]          65,664\n",
      "           Conv1d-66            [-1, 128, 3072]         491,648\n",
      "             ReLU-67            [-1, 128, 3072]               0\n",
      "           Conv1d-68            [-1, 128, 3072]         245,888\n",
      "             ReLU-69            [-1, 128, 3072]               0\n",
      "  ConvTranspose1d-70             [-1, 64, 6144]          16,448\n",
      "           Conv1d-71             [-1, 64, 6144]         122,944\n",
      "             ReLU-72             [-1, 64, 6144]               0\n",
      "           Conv1d-73             [-1, 64, 6144]          61,504\n",
      "             ReLU-74             [-1, 64, 6144]               0\n",
      "           Conv1d-75              [-1, 2, 6144]           1,922\n",
      "             ReLU-76              [-1, 2, 6144]               0\n",
      "         WaveUNet-77              [-1, 2, 6144]               0\n",
      "           Conv1d-78             [-1, 64, 3072]             448\n",
      "      BatchNorm1d-79             [-1, 64, 3072]             128\n",
      "             ReLU-80             [-1, 64, 3072]               0\n",
      "        MaxPool1d-81             [-1, 64, 1536]               0\n",
      "           Conv1d-82             [-1, 64, 1536]          28,672\n",
      "      BatchNorm1d-83             [-1, 64, 1536]             128\n",
      "             ReLU-84             [-1, 64, 1536]               0\n",
      "           Conv1d-85             [-1, 64, 1536]          12,288\n",
      "      BatchNorm1d-86             [-1, 64, 1536]             128\n",
      "             ReLU-87             [-1, 64, 1536]               0\n",
      "       BasicBlock-88             [-1, 64, 1536]               0\n",
      "           Conv1d-89             [-1, 64, 1536]          28,672\n",
      "      BatchNorm1d-90             [-1, 64, 1536]             128\n",
      "             ReLU-91             [-1, 64, 1536]               0\n",
      "           Conv1d-92             [-1, 64, 1536]          12,288\n",
      "      BatchNorm1d-93             [-1, 64, 1536]             128\n",
      "             ReLU-94             [-1, 64, 1536]               0\n",
      "       BasicBlock-95             [-1, 64, 1536]               0\n",
      "           Conv1d-96             [-1, 128, 768]          57,344\n",
      "      BatchNorm1d-97             [-1, 128, 768]             256\n",
      "             ReLU-98             [-1, 128, 768]               0\n",
      "           Conv1d-99             [-1, 128, 768]          49,152\n",
      "     BatchNorm1d-100             [-1, 128, 768]             256\n",
      "          Conv1d-101             [-1, 128, 768]           8,192\n",
      "     BatchNorm1d-102             [-1, 128, 768]             256\n",
      "            ReLU-103             [-1, 128, 768]               0\n",
      "      BasicBlock-104             [-1, 128, 768]               0\n",
      "          Conv1d-105             [-1, 128, 768]         114,688\n",
      "     BatchNorm1d-106             [-1, 128, 768]             256\n",
      "            ReLU-107             [-1, 128, 768]               0\n",
      "          Conv1d-108             [-1, 128, 768]          49,152\n",
      "     BatchNorm1d-109             [-1, 128, 768]             256\n",
      "            ReLU-110             [-1, 128, 768]               0\n",
      "      BasicBlock-111             [-1, 128, 768]               0\n",
      "          Conv1d-112             [-1, 256, 384]         229,376\n",
      "     BatchNorm1d-113             [-1, 256, 384]             512\n",
      "            ReLU-114             [-1, 256, 384]               0\n",
      "          Conv1d-115             [-1, 256, 384]         196,608\n",
      "     BatchNorm1d-116             [-1, 256, 384]             512\n",
      "          Conv1d-117             [-1, 256, 384]          32,768\n",
      "     BatchNorm1d-118             [-1, 256, 384]             512\n",
      "            ReLU-119             [-1, 256, 384]               0\n",
      "      BasicBlock-120             [-1, 256, 384]               0\n",
      "          Conv1d-121             [-1, 256, 384]         458,752\n",
      "     BatchNorm1d-122             [-1, 256, 384]             512\n",
      "            ReLU-123             [-1, 256, 384]               0\n",
      "          Conv1d-124             [-1, 256, 384]         196,608\n",
      "     BatchNorm1d-125             [-1, 256, 384]             512\n",
      "            ReLU-126             [-1, 256, 384]               0\n",
      "      BasicBlock-127             [-1, 256, 384]               0\n",
      "          Conv1d-128             [-1, 512, 192]         917,504\n",
      "     BatchNorm1d-129             [-1, 512, 192]           1,024\n",
      "            ReLU-130             [-1, 512, 192]               0\n",
      "          Conv1d-131             [-1, 512, 192]         786,432\n",
      "     BatchNorm1d-132             [-1, 512, 192]           1,024\n",
      "          Conv1d-133             [-1, 512, 192]         131,072\n",
      "     BatchNorm1d-134             [-1, 512, 192]           1,024\n",
      "            ReLU-135             [-1, 512, 192]               0\n",
      "      BasicBlock-136             [-1, 512, 192]               0\n",
      "          Conv1d-137             [-1, 512, 192]       1,835,008\n",
      "     BatchNorm1d-138             [-1, 512, 192]           1,024\n",
      "            ReLU-139             [-1, 512, 192]               0\n",
      "          Conv1d-140             [-1, 512, 192]         786,432\n",
      "     BatchNorm1d-141             [-1, 512, 192]           1,024\n",
      "            ReLU-142             [-1, 512, 192]               0\n",
      "      BasicBlock-143             [-1, 512, 192]               0\n",
      "AdaptiveAvgPool1d-144               [-1, 512, 1]               0\n",
      "          Linear-145                    [-1, 1]             513\n",
      "          ResNet-146                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 3,115,250,499\n",
      "Trainable params: 3,115,250,499\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 264.02\n",
      "Params size (MB): 11883.74\n",
      "Estimated Total Size (MB): 12147.78\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(1, 6184))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\FND\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.75 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\FND\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\FND\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\FND\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.75 GiB. GPU "
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(\"D:/code/best_model.pt\"))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"D:/code/best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"D:/code/best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "all_outputs = []  # 用于存储所有测试输出\n",
    "all_labels = []   # 用于存储所有标签\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        test_outputs = model(inputs)\n",
    "        \n",
    "        # 将当前输出和标签存储到列表中\n",
    "        all_outputs.append(test_outputs.cpu().numpy())  # 将输出移回 CPU 并转换为 NumPy 数组\n",
    "        all_labels.append(labels.cpu().numpy())          # 将标签移回 CPU 并转换为 NumPy 数组\n",
    "\n",
    "        loss = criterion(test_outputs.squeeze(), labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "# 计算平均损失\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "# 合并输出和标签\n",
    "all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_all_labels = label_std.inverse_transform(all_labels.reshape(-1, 1))\n",
    "inv_all_outputs = label_std.inverse_transform(all_outputs)\n",
    "\n",
    "pl.figure(figsize=(10, 5))\n",
    "pl.scatter(inv_all_labels, inv_all_outputs, alpha=0.5)\n",
    "pl.xlabel(\"True Mc\")\n",
    "pl.ylabel(\"Predicted Mc\")\n",
    "pl.title(\"Prediction of Mc\")\n",
    "pl.grid()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FND",
   "language": "python",
   "name": "fnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
